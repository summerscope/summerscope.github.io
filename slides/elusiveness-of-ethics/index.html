<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>The elusiveness of ethics: encoding fairness in an unfair world</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-background-image="img/Tactile_3.jpg">
					<h1>The elusiveness of ethics</h1> 
					<h3>Encoding fairness in an unfair world</h3>
				</section>
				<section>
					<h2>Agenda</h2>
					<ol>
						<li><strong>Talk</strong> (40 minutes)</li>
						<li><strong>Q &amp; A</strong> (10 minutes)</li>
						<li><strong>Break</strong> (5-10 minutes)</li>
						<li><strong>Activity</strong> (25-30 minutes)</li>
					</ol>
				</section>
				<section>
					<h2>About me</h2>
					<img class="plain" src="img/foundress.png" height="400px" />
					<aside class="notes">About me -  background, learning deep learning & ML, reading papers, Fair ML reading group</aside>
				</section>
				<section>
					<h2>Slides</h2>
					<p><small><a href="http://summerscope.github.io/slides/elusiveness-of-ethics" target="_blank">summerscope.github.io/slides/elusiveness-of-ethics</a></small></p>
					<aside class="notes">Links all live, don't stress about trying to write down URLs</aside>
				</section>
				<section>
					<section>
						<h1>Section 1</h1>
						<h3 class="fragment">Setting the scene</h3>
					</section>
					<section>
						<h2>Once upon a time, in a<br />
							FinTech startup...</h2>
						<aside class="notes">Time I worked at a robo investor. We had a risk appetite quiz - women locked out of top risk portfolio...</aside>
					</section>
					<section>
						<h2>Descriptive <br> -vs- <br> Normative</h2>
					</section>
					<section data-background-image="img/mean-girls.jpg">
						<h2>Girls wear pink</h2>
						<h3 class="fragment">(Descriptive)</h3>
					</section>
					<section data-background-image="img/sabrina.gif">
						<h2>Girls wear pink</h2>
						<h3 class="fragment">(Normative)</h3>
					</section>
					<section>
						<h2>The way the world is?</h2>
						<h2 class="fragment">or</h2>
						<h2 class="fragment">The way the world should&nbsp;be?</h2>
					</section>
					<section>
						<h2>For example...</h2>
					</section>
					<section data-background-image="img/drone.jpg">
						<aside class="notes">Drone assessing a bridge. It's observing the state of the world (damage to bridge) and the system makes a normative claim: we want to identify and repair damage to bridges! Cracks in bridges are observed in the world (due to various forces of attrition) but not accepted as the inevitable state.</aside>
					</section>
					<section>
						<h2>We make normative assertions all the time</h2>
						<aside class="notes">And it's normal, not interpreted as moralising or judgemental. But we can get squeamish about doing this when it comes to tech ethics. More on this soon.</aside>
					</section>
					<section>
						<h2>Think of the drone...</h2>
						<aside class="notes">If you ever get pushback on making a call like this, think of the drone. Someone designed a rule into the system which identified what was wrong that needed to be repaired. The judgement call is implied. It's the 'status quo' is not good rationale for non-intervention.</aside>
					</section>
					<section>
						<h2>Why should Machine Learning systems be more fair than the data from which they learn?</h2>
						<aside class="notes">Taken from a chat I had with a friend over drinks one night. It's a good question and one I think we need to grapple with to do this work.</aside>
					</section>
					<section>
						<h2>All data is bias</h2>
					</section>
					<section>
						<h2>Some bias is harmful</h2>
					</section>
					<section>
						<h2>Computers can't tell the difference</h2>
					</section>
					<section>
						<h2>ML systems can harden and amplify harmful bias</h2>
						<aside class="notes">Ask for examples of ML bias - give examples if audience doesn't know any.</aside>
					</section>
					<section>
						<h2>Why diversity?</h2>
						<ol>
							<li class="fragment">On principle: technology should belong to all</li>
							<li class="fragment">People who experience bias are more sensitive to its possibilities</li>
							<li class="fragment"><em>POC, Women, etc., are not inherently more ethical than anyone else</em></li>
						</ol>
						<aside class="notes">We need diversity and inclusion because people at the intersections of bias have a more finely tuned radar for the problems. If you experience it you can sniff it out more quickly. Women, people of colour, queer and lGtbQI people are not inherently better, more ethical, more innovative, etc, than white cis het men. They are, however, walking litmus tests for the worlds structural inequalities, and that experience is extremely useful for this work.</aside>
					</section>
					<section>
						<h2>Human bias (implicit or explicit) is scoped, the impact limited in a way that machine bias is not</h2>
					</section>
					<section data-background-image="img/janet.gif"></section>
					<section>
						<h2>Issue of scale is also the <br>promise of scale</h2>
						<aside class="notes">One of the shortest routes to the techno-utopian vision is to use the urgency of impact to force us to tackle the work of ethics for ourselves (as individuals) as well as for the machines.</aside>
					</section>
					<section data-background-image="img/janet2.gif">
						<aside class="notes">Technology is fundamentally neutral. It's not moral, or immoral, it's a-moral. Technology is a blank canvas, we are the painters, and the art is all ours.</aside>
					</section>
					<section>
						<h2>Why should we use machines to solve a problem caused by machines?</h2>
						<aside class="notes"></aside>
					</section>
					<section>
						<h2>We need technical solutions to solve technical problems</h2>
						<h1>at scale</h1>
						<aside class="notes"></aside>
					</section>
					<section>
						<h2>For example... </h2>
						<img class="plain" src="img/google-photos.png" alt="Google Photos" />
						<aside class="notes">Example of amount of human time needed to assess and make a judgement about ML classifications from a single model.</aside>
					</section>
					<section>
						<ul>
							<li><strong>1.2 billion</strong> photos per day uploaded to Google photos in 2017</li>
							<li class="fragment">Manually labelling each one would take well over 1 million people working full time</li>
							<li class="fragment">If <strong>1%</strong> of those label decisions were flagged for human review and each review took <strong>3 minutes</strong>, it would require <strong>150,000</strong> people working full time (Google currently employs 103,459 people)</li>
						</ul>
					</section>
					<section>
						<h2>We need to bake in tests and set up thresholds</h2>
						<h2 class="fragment">Save human attention for the trickiest cases</h2>
					</section>
					<section>
						<h2>"Machine assisted fairness"</h2>
						<aside class="notes">You heard it here first!</aside>
					</section>
					<!-- <section>
						<h2>Computers are great for...</h2>
						<ul>
							<li>Following defined rules</li>
							<li>Stopping at short-circuits</li>
							<li>Computing at scale</li>
						</ul>
						<aside class="notes"></aside>
					</section>
					<section>
						<h2>Humans are great for...</h2>
						<ul>
							<li>Pulling the emergency break</li>
							<li>Additional review of tricky use-cases</li>
							<li>Understanding the human impact of a decision</li>
						</ul>
						<aside class="notes"></aside>
					</section> -->
				</section>
				<section>
					<section>
						<h1>Section 2</h1>
						<h3 class="fragment">Approaches</h3>
						<aside class="notes">What are the ways people are tackling this problem? I divide it into (roughly) three big categories.</aside>
					</section>
					<section>
						<h2>1. Tooling &amp; Tests</h2>
						<aside class="notes">Dev tools, code tests. Looking to mathematical definitions of fairness to help us build dev tools.</aside>
					</section>
					<section>
						<h2>Where might we try to address bias?</h2>
						<ol>
							<li>Capturing and labelling data</li>
							<li>ML design (modeling decisions)</li>
							<li>Retroactively after classification</li>								
						</ol>
					</section>
					<!-- Proposed ideas like labels and certifications for training data. -->
					<section>
						<ul>
							<li><a href="http://aix360.mybluemix.net" target="_blank">AI Explainability 360</a></li>
							<li><a href="https://aif360.mybluemix.net" target="_blank">AI Fairness 360</a></li>
							<li><a href="https://github.com/marcotcr/lime" target="_blank">Lime</a></li>
							<li><a href="" target="_blank"></a></li>
							<li><a href="" target="_blank"></a></li>
							<li></li>
							<li></li>
							<li>Unit tests?</li>
							<li>CI - "Continuous Inference" - "CAI"</li>
						</ul>
						<aside class="notes"></aside>
					</section>
					<section>
						<h2>2A. Academic research</h2>
						<aside class="notes">Conferences, papers, ML research</aside>
					</section>
					<section>
						<ul>
							<li>FAT (fairness, transparency, accountability)</li>
							<li><a href="https://fairxiv.org" target="_blank">fairxiv.org</a></li>
							<li><a href="" target="_blank"></a></li>
							<li>FAT ML</li>
							<li></li>
						</ul>
						<aside class="notes">State of academia (as far as I can tell): 1 step forward, 2 steps backwards</aside>
					</section>
					<section>
						<h1>For example</h1>
						<p class="fragment">Trying to remove gender bias from NLP</p>
						<aside class="notes"></aside>
					</section>
					<section>
						"Man is to Computer Programmer as Woman is to Homemaker?
						Debiasing Word Embeddings"
						<p><a href="https://arxiv.org/pdf/1607.06520.pdf" target="_blank">https://arxiv.org/pdf/1607.06520.pdf</a></p>
						<aside class="notes">Where they tried to flatten (remove) the gender dimension</aside>
					</section>
					<section>
						<h2>Lipstick on a pig</h2>
						<img class="plain" src="img/lipstick.png" height="400" />
						<p><a href="https://arxiv.org/pdf/1903.03862.pdf" target="_blank">https://arxiv.org/pdf/1903.03862.pdf</a></p>
						<aside class="notes"></aside>
					</section>
					<section>
						<h2>2B. Self Regulation /<br> Soft Regulation</h2>
						<aside class="notes">Everything that isn't dev tools, research or law</aside>
					</section>
					<section>
						<ul>
							<li>Standards</li>
							<li>Checklists</li>
							<li>Policies</li>
							<li>Risk assessment frameworks</li>
						</ul>
						<aside class="notes">Hmmm guess what's in common? They're all after the fact</aside>
					</section>
					<section>
						<p><a href="https://ai-hr.cyber.harvard.edu/primp-viz.html" target="_blank">Principled artificial intelligence visualisation</a></p>
						<iframe name="PRIMP" src="https://ai-hr.cyber.harvard.edu/primp-viz.html" width="700" height="500" frameborder="0" scrolling="auto" class="frame-area"></iframe>	
					</section>
					<section>
						<h2>Special mentions</h2>
					</section>
					<section>
						<h2>The Montreal Declaration</h2>
						<p><a href="https://www.montrealdeclaration-responsibleai.com/" target="_blank">montrealdeclaration-responsibleai.com</a></p>
						<aside class="notes">Developed out of U of Montreal. Supported by MILA (Geoffrey Hinton)</aside>
					</section>
					<section>
						<h2>3. Law / Hard Regulation</h2>
						<ul>
							<li>GDPR</li>
							<li><em>National agency?</em></li>
							<li><em>Right to a fair explanation?</em></li>
						</ul>
						<aside class="notes"></aside>
					</section>
					<section>
						<ul>
							<li><a href="" target="_blank"></a></li>
							<li></li>
							<li><a href="" target="_blank"></a></li>
							<li></li>
						</ul>
						<aside class="notes"></aside>
					</section>
					<section>
						Slide 5
						<aside class="notes"></aside>
					</section>
				</section>
				<section>
					<section>
						<h1>Section 3</h1>
						<h3 class="fragment">Fairness definitions</h3>
					</section>
					<section>
						Slide 5
						<aside class="notes"></aside>
					</section>
					<section>
						Slide 5
						<aside class="notes"></aside>
					</section>
					<section>
						<h2>Fairness definitions cheatsheet</h2>
						<p><a href="https://tinyletter.com/summerscope" target="_blank">tinyletter.com/summerscope</a></p>
					</section>
				</section>
				<section>
					<section>
						<h1>Section 4</h1>
						<h3 class="fragment">Final hot takes</h3>
					</section>
					<section>
						<h2>There is no prediction; only intervention</h2>
						<aside class="notes">Can anyone give an example of building a prective model which didn't have any intended agency or intervention in the world? We predict things in order to do something about them.</aside>
					</section>
					<section>
						<h2>Classification is never descriptive, always normative</h2>>
						<aside class="notes">The boy and the bundle of sticks? Everything we tell people is a story they learn about themselves. Even if they reject the story, it becomes ingraned in their identity.</aside>
					</section>
					<section>
						<h2>No UI? Still design</h2>
						<aside class="notes"></aside>
					</section>
					<section>
						Slide 5
						<aside class="notes"></aside>
					</section>
					<section>
						Slide 5
						<aside class="notes"></aside>
					</section>
					<section>
						Slide 5
						<aside class="notes"></aside>
					</section>
				</section>
				<section>
					<section>
						<h1>Section 5</h1>
						<h3 class="fragment">subtitle</h3>
					</section>
					<section>
						Slide 5
						<aside class="notes"></aside>
					</section>
					<section>
						Slide 5
						<aside class="notes"></aside>
					</section>
					<section data-background-image="img/Tactile_9.jpg">
						<h2>Acknowledging complexity</h2>
						<aside class="notes">I’m not going to pretend that this is easy or that the work is clear cut and unambiguous. It’s not. But all the same it’s real engineering work. It’s about identifying trade offs, working through use cases, and making difficult compromises. It’s about staring unflinchingly at complex systems and refusing to blink. </aside>
					</section>
				</section>
				<section>
					<section data-background-color="#ffffff" data-background-image="img/Tactile_7.jpg">
						<h1>Thank you!</h1>
						<h3>Questions?</h3>
						<p><small><a href="http://summerscope.github.io/slides/elusiveness-of-ethics" target="_blank">summerscope.github.io/slides/elusiveness-of-ethics</a></small></p>
					</section>
					<section>
						<h2>Further reading</h2>
						<ul>
							<li></li>
							<li></li>
							<li></li>
							<li></li>
							<li></li>
							<li></li>
						</ul>
					</section>
				</section>
				<section>
					<section>
						<h1>Activity</h1>
					</section>
					<section>
						<h2>A lean ethics canvas</h2>
						<aside class="notes">See who's done a lean business canvas.</aside>
					</section>
					<section>
						<a href="https://hbr.org/2018/04/a-simple-tool-to-start-making-decisions-with-the-help-of-ai" target="_blank">
							<img class="plain" src="img/deep-learning-canvas.png" height="600" />
						</a>
					</section>
					<section>
						<img class="plain" src="img/W180106_AGRAWAL_THEAI_v2.png" height="600" />
					</section>
					<section>
						<img class="plain" src="img/W180106_AGRAWAL_THEAIEXAMPLE.png" height="600" />
					</section>
					<section data-background-image="img/sabrina-snap.gif">
						<h2>Let's try it!</h2>
						<aside class="notes">skip out of slides to PDFS for activity. 3 minutes for initial brainstorm. 15 minutes to complete canvas in groups of 4. 5 minutes to share.</aside>
					</section>
				</section>
				<!-- <aside class="notes"></aside> -->
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				backgroundTransition: 'zoom', // none/fade/slide/convex/concave/zoom
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
