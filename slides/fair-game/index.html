<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<meta name="description" content="How do we connect high-level principles with day-to-day product decision making? How do we move past the AI Ethics hype and start trying, testing and implementing practical approaches? These questions are at the heart of Laura's work, and in this talk she shares stories, discoveries, and decisions from her time as an 'ethics ops' consultant embedded with a small team in a big telco. From improving the science bit of data science, to developing the collective sensitivity of the team, to designing recourse for false positives, tune in for pragmatic pointers and actionable take-aways that you can try with your team right away.">
		<title>Fair game: the ethics of telco fraud</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css" id="theme">
		<link href="https://fonts.googleapis.com/css2?family=Krub:wght@200;300;400;600&family=Lora:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-background-image="img/squares.png">
					<h1>Fair game</h1>
					<h3>The ethics of telco fraud</h3>
				</section>
				<section>
					<h2>Intro</h2>
					<aside class="notes">About me, ethics litmus tests, etc</aside>
				</section>
				<section>
					<h2>Ethics Litmus Tests</h2>
					<h2 class="fragment">now online!</h2>
					<aside class="notes">NB borrowed from Ben Buchanon (big stonking post) - V1 static, V2 is going to allow suggestions and a vote / poll mechanism to add new cards</aside>
				</section>
				<section>
					<h2>Slides</h2>
					<p><a href="http://summerscope.github.io/slides/fair-game/" target="_blank">summerscope.github.io/slides/fair-game</a></p>
				</section>
				<section>
					<section>
						<h1>Section 1</h1>
						<h2>Top-down vs. bottom-up</h2>
					</section>
					<section>
						<h2>Responsible tech</h2>
						<h3 class="fragment">No magic bullets</h3>
						<aside class="notes">
							if you're hoping to learn how to write an ethics strategy document, this is probably not the talk for you. I'm going to share anecdotes and experiences about my work with the messy intersection of people, data, and systems - but I'm won't offer any easy answers, or a prescription for success. 
						</aside>
					</section>
					<section>
						<h2>No magic bullets<sup>*</sup></h2>
						<h3 class="fragment"><sup>*</sup>Anyone who tells you otherwise is selling something</h3>
						<aside class="notes">There are no magic bullets. The desire for a magic bullet is part of the problem. We're in the Cambrian explosion phase not just of AI but of also of AI Ethics. It's on us to interrogate the power structures and agendas lurking behind the glossy marketing copy.</aside>
					</section>
					<section>
						<!-- img placeholder harvard map of principles docs -->
						<h2>On endless principles</h2>
						<aside class="notes">
							Let me be a bit more precise in my critique of strategy documents. I'm not particularly interested in writing ethics strategy or data governance strategy - whether industry, govt or company driven, unless it there is a clear line between the principle being defined and the granular decisions being made at a product or data level. The day-to-day decisions. The little course corrections.
						</aside>
					</section>
					<section>
						<blockquote>"As a result, developers are becoming frustrated by how little help is offered by highly abstract principles when it comes to the ‘day job’"*</blockquote>
						<p><small>- From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices
							https://arxiv.org/abs/1905.06876 </small></p>
						<aside class="notes">
							***Broad consensus on principles? So what? Just a dogs breakfast of ideas that are hard to argue with in-principle and hard to test for in practice.
							"a lack of progress in the translation of available tools and methods from academic literature or early-stage development and research, to real-life use"*
						</aside>
					</section>
					<section>
						<blockquote>"Must be designed for human flourishing" [ref]</blockquote>
						<aside class="notes">"Cool story, what does that even mean? What's the code test for that?"</aside>
					</section>
					<section>
						<blockquote>"Cultivating diversity in our teams to ensure everyone has a seat on the table" [ref] </blockquote>
						<aside class="notes">Nice but how much diversity is enough? How will you measure that? Will you report on it - internally, or externally?</aside>
					</section>
					<section>
						<blockquote>"Avoid capturing personal data whenever possible" [ref]</blockquote>
						<aside class="notes">"Right but what if you need to capture sensitive personal data to know if your system has become biased against that attribute?"</aside>
					</section>
					<section>
						<blockquote>"A string of empty profunditites"</blockquote>
						<p>- Andy Kitchen</p>
						<aside class="notes">
							The work is in the detail, the daily decisions. The desire to abstract, to generalise, can itself be a form of procrastination.  
						</aside>
					</section>
					<section>
						<!-- img placeholder - throwing a molotov cocktail -->
						<aside class="notes">Don't point at a document, framework, or methodology and say "Go use it". Champion the approach from inside the team. Get your hands dirty. Show by doing.  </aside>
					</section>
					<section>
						<h2>Get your hands dirty</h2>
						<h3>Tangle in the grey</h3>
						<aside class="notes">Advice to consultants - don't other yourself or other your work! Skill sharing, exploring platforms, tech approaches, etc - all better if you try to join the team rather than provide outside, othered advice. You'll better understand the texture of their problems. You'll also have a better sense of whether your proposed solution is viable. </aside>
					</section>
					<section>
						<!-- img placeholder attention thief -->
						<aside class="notes">
							Without measurable outcomes, these principles are toothless. And worse, they're resource thiefs. They suck the oxygen from the room. They lull us into thinking we're already doing the work, without actually having started. I'd argue the real work starts at implementation. What's your plan to integrate these principles into your decision making at every level? How will this change your culture? How will you help people unlearn bad habits? Who will set the boundaries of acceptable time and discourse (e.g. how much money are you willing to spend)? Do you have internal champions who can 'show by doing'? Do you need to offer training or upskilling? 
						</aside>
					</section>
					<section>
						<h2>Commit to action</h2>
						<ul>
							<li class="fragment">set measurable targets</li>
							<li class="fragment">provide examples</li>
							<li class="fragment">allow for uncertainty</li>
						</ul>
						<aside class="notes">
							If you must have a strategy document or page, make it a commitment to action. Without real commitment to the longevity of your principles, what often happens is the documents gather dust in the corner, and life very quickly returns to the status quo.
						</aside>
					</section>
					<section>
						<!-- img placeholder - driving fast on the audobon -->
						<aside class="notes">
							Or to frame it another way: rather than thinking we can reduce the risks of our technology with a strategy document, can we acknowledge the risks and ask everyone to take them seriously and *not* pretend to have solved them? A metaphor: driving fast on the autobahn - the German freeway that has no speed limit. It's scary, but also exhilarating, and you're alert to the danger. Sometimes we *want* to feel the wind rushing past. Perhaps insulating ourselves from that sensation is unhelpful. 
						</aside>
					</section>
					<section>
						<blockquote>“Never doubt that a small group of thoughtful, committed, citizens can change the world. Indeed, it is the only thing that ever has.”</blockquote>
						<p>-Margaret Mead</p>
						<aside class="notes">
							Because to be clear, you don't really need principles, policy, or values to get started. You just need a few bright sparks and enough rope. So let's tangle with the grey. 
						</aside>
					</section>
				</section>
				<section>
					<section>
						<h1>Section 2</h1>
						<h2>Friend or fraud?</h2>
					</section>
					<section>
						<h2>Joining a telco</h2>
						<aside class="notes">
							At the beginning of 2020 (which is either a year or a lifetime ago, who even knows) I joined a fraud detection team in a telco. I'd describe my role as an 'ethics ops' consultant, but I don't think that's the formal title - their recruiting systems didn't have a role that matched. The team - 'pod' in the internal parlance - was relatively new and had been spun up in response to some fairly serious fraud attacks that had taken a bite out of their revenue. The approach to fraud detection is pretty standard - a mix of monitoring, internal business rules, and using third party systems with proprietary fraud algorithms.
						</aside>
					</section>
					<section>
						<!-- img placeholder eye glasses with labels user harm,  implicit bias, explicit bias -->
						<aside class="notes">
							My job is to introduce a harm identification and mitigation lens to the work. Fraud is an interesting space within which to discuss ethics, because not only are you (the company) capable of inflicting harm on your customers, you are also under attack by a sub-set of customers who mean you harm.
						</aside>
					</section>
					<section>
						<h2>A balancing act</h2>
						<aside class="notes">
							You have to balance the real, often urgent concerns of identifying bad behaviour and halting costs while also acknowledging the possibility of false positives, and considering how you might identify and support any innocent parties who accidentally get caught in the cross-fire. Add to this the ever-changing space of fraud activity. Email spam is a similar field - just as the algorithms get smarter at detecting spam, the spammers get cleverer in their approaches to slipping the net. 
						</aside>
					</section>
					<section>
						<h1 class="learning">Finding #1</h1>
						<h2>Data (interpretation) pride comes before a fall</h2>
						<aside class="notes">
						</aside>
					</section>
					<section>
						<h2>For example</h2>
						<aside class="notes">
							The textivist... The auto-call on smartwatch... 
						</aside>
					</section>
					<section>
						<aside class="notes">
							LOOK at your raw data! Let lots of people look at it. Ask yourself questions. You'll be surprised at what SME and different stakeholders will be able to tell you. 
						</aside>
					</section>
					<section>
						<h2>Deconstructing your proxy</h2>
						<aside class="notes">All data is a proxy - it's an approximation of the thing we actually want to measure</aside>
					</section>
					<section>
						<h2>A concept framework</h2>
						<!-- img placeholder illustration of this flow  -->
						<ul>
							<li class="fragment">Signal</li>
							<li class="fragment"><em>infers</em> Behaviour</li>
							<li class="fragment"><em>infers</em> Persona</li>
						</ul>
						<p class="fragment">Don't forget we also measure anti-signals</p>
						<aside class="notes">
							This is the framing I use, and I think it helps us be explicit about the logic leaps we are making. Always asking the question how good a proxy is this data for the thing I REALLY want to know?
						</aside>
					</section>
					<section>
						<!-- placeholder - give an example of how this model works with a real data source -->
					</section>
					<section>
						<h1 class="learning">Finding #2</h1> 
						<h2>Develop a healthy cynicism about data sources</h2>
						<aside class="notes">
							This is really an extension of the first learning / point. 
						</aside>
					</section>
					<section>
						<h2>Anecdote</h2>
						<!-- img placeholder early nuclear detection system or sunlight on clouds? -->
						<aside class="notes">
							Russian tech looking at radar, had a rule to launch missile but didnt.
							This is usually held up as a story about humanity but I think it's also a story about the deep intuition of a technician. Someone who can put their hand on the bonnet and know how well the machine is running.
						</aside>
					</section>
					<section>
						<h2>Look at your raw data</h2>
						<h3 class="fragment">Not just at dashboards</h3>
						<aside class="notes">Hopefully not controversial point! We should want to get up close and personal with our incoming data. Ask ourselves what it tells us. Observe where it looks clean and where the types are inconsistent or muddy. You'll be surprised how much you learn from just skimming a few hundred rows in a raw table. </aside>
					</section>
					<section>
						<blockquote>Data misinterpretation is <u>easy</u>, and <u>likely</u> under pressure</blockquote>				
						<aside class="notes">One of the most interesting things about working with fraud is the pressure it puts on data interpretation in periods of attack. Like putting data science hygiene through the wringer -  in unexpected bursts. </aside>
					</section>
					<section>
						<h2>For example...</h2>
						<aside class="notes">
							3 hour batch timeframe. Maintenance outages (planned) causing delays in data stream. 
						</aside>
					</section>
					<section>
						<h2>Design for cognitive load</h2>
						<aside class="notes">
							Wherever possible, add labels. Be explicit. Make it impossible to misinterpret.
						</aside>
					</section>
					<section>
						<h2>Simple doesn't imply you're stupid</h2>
						<aside class="notes">
							Just because your team is smart, technical, doesn't mean you want them to have to be brilliant detectives under time and cost pressure. 
						</aside>
					</section>
					<section>
						<h2>Litmus test</h2>
						<h3>On a quick glance, is this easy to misunderstand?</h3>
						<aside class="notes">
							Even if you're not working in the space of fraud, we experience this same kind of pressure all the time. When a customer discovers a bug in production. When there's an outage with a third party, etc. 
						</aside>
					</section>
					<section>
						<h2>Pointers for data interpretation</h2>
						<ul>
							<li class="fragment">Using DSL (domain specific language)?</li>
							<li class="fragment">Absolute (total count) or relative (%)?</li>
							<li class="fragment">Is the data recency clear?</li>
						</ul>
					</section>
					<section>
						<h1 class="learning">Finding #3</h1> 
						<h2>Holding space for uncertainty <br>(Without being paralysed)</h2>
						<aside class="notes">We want to acknowledge the uncertainty inherent in our decision making without allowing us to be paralysed</aside>
					</section>
					<section>
						<h2>Probability theory</h2>
						<aside class="notes">Talking about probabilities in a cross-functional team is... hard. Everyone will have a different level of stats and data literacy. Acknowledge this. Start with simple ways of observing and discussing confidence, don't jump into the deep end. </aside>
					</section>
					<section>
						<h2>For example</h2>
						<!-- img placeholder for a few simple ways of measuring confidence  -->
						<aside class="notes"></aside>
					</section>
					<section>
						<h2>Changing our language changes our minds</h2>
						<blockquote class="fragment">"We'll wait until we're 100% certain... no make that 99% certain"</blockquote>
						<aside class="notes">Colleague who has started to change his language </aside>
					</section>
					<section>
						<h2>Deterministic vs Stochastic</h2>
						<h3 class="fragment">There will be noise either way</h3>
						<aside class="notes">A lot is made about the difference between deterministic programming (where the code is fixed) and stochastic. But in any complex system there will be noise, and unexpected effects. Any automation should expect to grapple with unintended outcomes, false positives. </aside>
					</section>
					<section>
						<h2>Machine Learning <em>is</em> probibalistic</h2>
						<aside class="notes">If you are bulding ML, the ONLY certainty is that some of the engine's outputs will be wrong. If you can't afford to design for decision review and decision recourse then you can't afford to design an ML model to use in prod.</aside>
					</section>
					<section>
						<!-- placeholder book on Noise & Bias in ML - Judea Pearl  -->
					</section>
					<section>
						<h2>Be explicit</h2>
						<h3 class="fragment">If you can't think of the consequences you're not thinking hard enough</h3>
						<aside class="notes">The best way to prepare yourself to design in this world is to be ultra explicit about the goals of your system, and the possible harms that might occur to your end users. </aside>
					</section>
					<section>
						<h2>Shot</h2>
						<blockquote>We believe this system has contributed to making Facebook the safest place on the Internet for people and their information.</blockquote>
						<p><a href="https://research.fb.com/publications/facebook-immune-system/" target="_blank"><small>Facebook Immune System by Tao Stein, Roger Chen, Karan Mangla</small></a></p>
						<aside class="notes">FB Research paper from 2011. :Screams_forever: </aside>
					</section>
					<section>
						<h2>Chaser</h2>
						<blockquote>"The goal is to protect the graph against all attacks rather than to maximize the accuracy of any one specific classifier. The opportunity cost of refining a model for one attack may be increasing the detection and response on other attacks."</blockquote>
						<p><a href="https://research.fb.com/publications/facebook-immune-system/" target="_blank"><small>Facebook Immune System by Tao Stein, Roger Chen, Karan Mangla</small></a></p>
						<aside class="notes">:Screams_forever: </aside>
					</section>
					<section>
						<h2>For example</h2>
						<aside class="notes">In the land of fraud the goal is to prevent the costs of service misuse and fraud. The harms are when we take action that can result in a user unexpectedly losing service. </aside>
					</section>
					<section>
						<h2>Litmus test</h2>
						<h3 class="fragment">What if this happened to my most vulnerable customer?</h3>
						<aside class="notes">I find this question useful for thinking about service level actions. It helps frame the cost of false positives and makes us more focused on the minimum viable intervention needed to achieve our goals of capping these costs.</aside>
					</section>
					<section>
						<h2>Added uncertainty in the space of fraud</h2>
						<h3 class="fragment">You <em>really</em> can't trust what people say</h3>
						<aside class="notes">In UXR you develop a sniff test for honesty and vulnerability vs bulshit answers. But in the fraud space you had this added complexity of knowing that sometimes customers may be lying or trying to game the system to meet their own ends. Dealing with this day-in day-out can make these discussions feel instantly suspect, and you'll start to see fraud everywhere. </aside>
					</section>
					<section>
						<h2>The antidote to suspicion</h2>
						<h3 class="fragment">Setting the intention to be respectful</h3>
						<aside class="notes">The best way to avoid falling into this trap is to be explicitly, deliberately kind and respectful in all our communications</aside>
					</section>
					<section>
						<!-- img placeholder pearl clutcher  -->
						<h2>bUt wHAt iF iT's A BAd AcTor?</h2>
						<h3 class="fragment">So what?</h3>
						<aside class="notes">What is the harm in being overly respectful to someone who is trying to rip you off? Versus what's the harm in speaking harshly to a legit customer by accident? Have to think about reputational harms, and stay focused on the goal.</aside>
					</section>
					<section>
						<h2>Keep the moralising <u>out of it</u></h2>
						<aside class="notes">Being 'right' isn't helpful. You don't know that person, you don't know their story. </aside>
					</section>
					<section>
						<h2>Tip</h2>
						<h3>Describe <em>behaviours</em>, not <em>people</em></h3>
						<aside class="notes">Describe behaviour (service misuse) not people (fraudster). There's literally no value in applying a moralising title to a person. THere's every possibility of harm, on both sides, if you get it wrong.</aside>
					</section>
					<section>
						<h1 class="learning">Finding #4</h1>
						<h2>Mapping knowledge states</h2>
						<aside class="notes">ok. so you probably knew this was coming. This is the section where i tell you that the best way to improve your data science practice as a whole is to do better, much better, at the boring science bits.</aside>
					</section>
					<section>
						<h2>Beware the swamp of lazy assumptions</h2>
						<!-- img placeholder swamp with lots of nasty things that can get you  -->
						<aside class="notes">Anecdote: when joined telco there was a lot of confusion about what types of fraud were, and what they were called. Names were quite confusing, unintuitive, imprecise language, name overlaps! I wasn't the only one who was confused.</aside>
					</section>
					<section>
						<h2>Implicit vs Explicit knowledge</h2>
						<h3 class="fragment">Become a knowledge excavator</h3>
						<aside class="notes">Just because a thing is knowable, doesn't mean it is known. Often the first step to establishign a shared understanding is to sniff out this existing implicit knowledge and formalise it.  </aside>
					</section>
					<section>
						<h2>Naming matters</h2>
						<ul>
							<li class="fragment">Build a shared vocabulary</li>
							<li class="fragment">Build shared mental models</li>
							<li class="fragment">Avoid name-space clashing</li>
						</ul>
					</section>
					<section>
						<h2>For example</h2>
						<blockquote>International revenue sharing fraud / toll fraud</blockquote>
						<p class="fragment">these are confusingly different names for the same thing</p>
						<p class="fragment">unrelated to Call Reselling (but these often get confused)</p>
					</section>
					<section>
						<h2>Personas</h2>
						<!-- img placeholder - one of the personas  -->
						<aside class="notes">Talk about writing personas documentation to have a shared model that stakeholders from support and ops through to data science can reference</aside>
					</section>
					<section>
						<h2>Defining your baseline</h2>
						<aside class="notes">If you don't have a shared understanding it's hard to move forwards. Needed for all experiments, hypotheses you want to try moving forwards. Also - perhaps minor discrepancies in what is the current state will reveal a question to investigate? </aside>
					</section>
					<section>
						<h2>For example</h2>
						<h3>Looking at the logs</h3>
						<aside class="notes">Looking at the history of what actions had been taken on a service number - can make longitudinal analysis very difficult if the reasons aren't well documented AND if there is no way to observe the occurance of false positives. If you don't know when you automation was wrong, hard to use that data with confidence. Also - maybe your logs aren't the source of truth, ok! as long as somewhere there is a source of truth. </aside>
					</section>
					<section>
						<h1 class="learning">Finding #5</h1>
						<h2>If you don't ask,<br /> customers won't tell</h2>
						<aside class="notes">Probably obvious! But important. Follows-on nicely from thinking about logs and improving our understanding of history. Most people won't bother to give you feedback.  If you want to know whether you've had false positives you need to allow for this possibility and design feedback loops in by default. </aside>
					</section>
					<section>
						<!-- img placeholder 'too hard basket' -->
						<aside class="notes">I think of this as the rule of lurkers vs contributors. In the space of fraud, if you have taken an action on a service they're more likely to churn than to make an effort to reach out to you - and why would they bother based on a market where churn is the norm and it's easy to swap providers? If they are really angry you might get feedback - but through the form of a complaint to the TIO (the regulator) - also a bad outcome. </aside>
					</section>
					<section>
						<h2>Customers <u>are</u> the experts...</h2>
						<h3 class="fragment">...on themselves</h3>
						<aside class="notes">If your system makes an incorrect assumption, are you in a position to find out?</aside>
					</section>
					<section>
						<h2>Feedback loops must be </h2>
						<ul>
							<li>Intuitive</li>
							<li>Contextual</li>
							<li>Timely</li>
						</ul>
						<aside class="notes">I'm a fan of micro-feedback, small and contextual questions that are really lightweight and easy to answer.</aside>
					</section>
					<section>
						<h2>Plan time for...</h2>
						<ul>
							<li>Customer support</li>
							<li>Product/model improvements</li>
							<li>Integrating your learnings</li>
						</ul>
					</section>
					<section>
						<h2>Gotchas</h2>
						<aside class="notes">If you take action to block or disconnect a service based on fraud - can't call or text that customer now! (ouch). And if you send your comms from a donotreply email, you make email replies difficult and discouraging (double ouch). </aside>
					</section>
					<section>
						<h1 class="learning">Finding #6</h1>
						<h2>Design for failure first</h2>
						<aside class="notes">Thinking about asking customers if a classification, prediction or automated decision impacting them was wrong flows nicely into this idea of designing for failure first.</aside>
					</section>
					<section>
						<h2>Your system <u>will</u> get it wrong</h2>
						<h3 class="fragment">So how will it handle failure?</h3>
						<aside class="notes">Start from the assumption that your system is not perfect. We know this is the case with any machine learning or statistical models. The literal only assumption you can make is that it will be wrong for some percent of the time. And ever removing the issue of statistical models there are plenty of other aspects of your tech stack and human stack that point to the strong certainty of some amount of failure: complexity, culture, etc.</aside>
					</section>
					<section>
						<h2>There are always consequences...</h2>
						<!-- placeholder paper reference - glen - FB engineers & false positives  -->
					</section>
					<section>
						<h2>Harm mapping</h2>
						<h3 class="fragment">Recommended starting point</h3>
						<p class="fragment"><a href="https://github.com/summerscope/mapping-fair-ml" target="_blank">github.com/summerscope/mapping-fair-ml</a></p>
						<aside class="notes">This is a whole topic but just to point to it, this is the bit where we think about what we might get wrong, and what are the consequences. There are a whole bunch of tools and frameworks out there to try - ethics litmus tests among them.</aside>
					</section>
					<section>
						<h2>Designing an escape hatch</h2>
						<!-- placeholder image escape hatch  -->
						<ul>
							<li class="fragment">email / sms template copy</li>
							<li class="fragment">support scripts for conversations</li>
							<li class="fragment">data schema design - capturing your learnings</li>
						</ul>
						<aside class="notes">Think of this as designing your escape hatch. You don't want to have to use it much - but you sure want it well oiled if you do have to use it.</aside>
					</section>
					<section>
						<h2>Prepare for pushback</h2>
						<ul>
							<li class="fragment">People don't like it when you make bad assumptions about them!</li>
							<li class="fragment">Making the implicit explict is going to be uncomfortable</li>
						</ul>
					</section>
					<section>
						<h2>Litmus test</h2>
						<h3>Could any customer go through my escape hatch, recover and remain a happy customer?</h3>
						<aside class="notes">Sometimes you have to think creatively - pitch a friction as a value add. "We are working hard to protect your identity and your account's safety" is a good one, for instance. Test out your explanations and flows on random people NOT those who match the assumptions your system made as a way to assess the quality of the solution.</aside>
					</section>
					<section>
						<h1 class="learning">Finding #7</h1>
						<h2>Speed matters</h2>
						<aside class="notes">Here's the classic problem in product - how fast should we build and deploy things</aside>
					</section>
					<section>
						<h2>Finding the sweet spot</h2>
						<!-- img placeholder spectrum between move fast and break things, and design by committeeeeeee -->
						<aside class="notes">Too slow can be just as dangerous as too fast</aside>
					</section>
					<section>
						<blockquote>Placeholder quote on speed of feedback loops</blockquote>
						- Donna meadows paper - systems interventions 
					</section>
					
					<section>
						<h2>Dangers of too slow</h2>
						<ul>
							<li class="fragment">Feedback takes too long</li>
							<li class="fragment">You may not see the impact of a decision</li>
							<li class="fragment">Doing things <em>feels hard</em></li>
							<li class="fragment">Culture of resistance</li>
							<li class="fragment">Diffusion of responsibility</li>
							<li class="fragment">Get anchored to your first idea</li>
							<li class="fragment">Sunk cost bias</li>
						</ul>
						<aside class="notes">Much more difficult to treat solutions / features / ideas lightly, as hypotheses to be validated or not. </aside>
					</section>
					<section>
						<h2>Dangers of too fast</h2>
						<ul>
							<li class="fragment">Ask forgiveness not permission</li>
							<li class="fragment">Insufficient knowledge / expertise</li>
							<li class="fragment">Overconfidence</li>
							<li class="fragment">Failing to measure impact</li>
							<li class="fragment">Reinventing the wheel</li>
						</ul>
						<aside class="notes">"The Facebook" (especially early years)</aside>
					</section>
					<section>
						<h1 class="learning">Finding #8</h1>
						<h2>Upstream prevention vs downstream mitigation</h2>
						<aside class="notes">What can you do in the product? Vs what do you try to intervene on after it happens? </aside>
					</section>
					<section>
						<blockquote>
							These sorts of things will happen. I’m very intentional about not saying abuse "might" happen - if it can, it will.
						</blockquote>
						- Eva PenzeyMoog <a href="https://evapenzeymoog.substack.com/p/coming-soon" target="_blank">newsletter</a>
						<aside class="notes">I see a reseblance between Fraud and Abuse (tech used in DV / DA). These are people with a different agenda to yours. They are looking for vulnerabilities they can exploit for their own ends.  </aside>
					</section>
					<section>
						<h2>Product use cascade</h2>
						<ol>
							<li class="fragment">What you <u>can do</u> in the product</li>
							<li class="fragment">Intentions, framing, design </li>
							<li class="fragment">Culture of your community</li>
							<li class="fragment">...</li>
							<li class="fragment">Terms of use</li>
						</ol>
					</section>
					<section>
						<h2>Possible = permissible</h2>
						<aside class="notes">I can't say it more strongly. If you don't care to make it impossible to do something in your product, that's tacit permission for the behaviour to occur. Even if it's not intended, designed for, etc. It sucks, but it's the nature of the beast.</aside>
					</section>
					<section>
						<h2>Setting boundaries <u>is design</u></h2>
						<aside class="notes">I we often miss out on opportunities to do this well. Marketing, on-boarding, engagement comms - these are all opportunities to show how people can and should be successful using your product, and to offer little nudges away from the kinds of behaviour you don't want to see.</aside>
					</section>
					<section>
						<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The rise and rise of terms of service is a genuinely astonishing cultural dysfunction. Think of what a bizarre pretense we all engage in, that anyone, ever, has read these sprawling garbage novellas of impenetrable legalese.<br><br>1/ <a href="https://t.co/4woefKaXDF">pic.twitter.com/4woefKaXDF</a></p>&mdash; Cory Doctorow #BLM (@doctorow) <a href="https://twitter.com/doctorow/status/1320399417979985920?ref_src=twsrc%5Etfw">October 25, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
						<aside class="notes">Think of TOC - necessary evil but not the intuitive place to enact change if it's possibel to go up the cascade</aside>
					</section>
					<section>
						<h2>Human-centred T&amp;Cs</h2>
						<aside class="notes">This space is growing and I'm excited to see developments in this space. Just don't expect this to replace the work of establishing guardarails/norms in your product design.</aside>
					</section>
					<section>
						<h2>Downstream is usually more costly than upstream</h2>
						<aside class="notes">In retail - think promotions abuse, etc. Can you add guardrails in at the planning stage? Finding and dealing with behaviour sucks. Anyone who's worked on an abuse team will testify to this. It's not always possible, but if you can resolve something with a product change rather than monitoriing for it and remediation after the fact - usually preferable.</aside>
					</section>
				</section>
				<section>
					<section>
						<h1>Section 3</h1>
						<h2>Final thoughts</h2>
					</section>
					<section>
						<h2>Rethinking automation design</h2>
						<aside class="notes">So many options we don't need to think about automations as all or nothing</aside>
					</section>
					<section>
						<h2>Breaking down the monolith</h2>
						<ul>
							<li class="fragment">Concierge prototypes (human-as-bot)</li>
							<li class="fragment">Automations can be temporary or time-boxed</li>
							<li class="fragment">Dashboard? Notification? Escalation? Action?</li>
							<li class="fragment">Do usability testing (duh)</li>
						</ul>
						<aside class="notes">If it feels weird/wrong as a human interaction, it probably means it's bad as an automation.</aside>
					</section>
					<section>
						<h2>#CultureGoals</h2>
					</section>
					<section>
						<h1 class="learning">Sniff test</h1>
						<h2>Magic bullet thinking</h2>
						<!-- img placeholder quotes and chat bubbles  -->
						<aside class="notes">Does it sound too good to be true? We need to build up our collective immunity to bulshit. As I said at the beginning - if someone is telling you they have a simple solution to a wicked problem, they're selling you something.</aside>
					</section>
					<section>
						<h1 class="learning">Sniff test</h1>
						<h2>Handballing</h2>
						<!-- img placeholder quotes and chat bubbles  -->
						<aside class="notes">This one usually isn't as simple as people handing off responsibility. More subtle, harder to pin down. Sitting on something so it doesn't progress. Flagging a concern but not supporting the work. </aside>
					</section>
					<section>
						<h1 class="learning">Sniff test</h1>
						<h2>Groupthink</h2>
						<!-- img placeholder quotes and chat bubbles  -->
						<aside class="notes">Get along isms. Vacuous cheerleading. When we're too focused on agreeing with each other to ask the hard questions.</aside>
					</section>
					<section>
						<h1 class="learning">Sniff test</h1>
						<h2>Rushing</h2>
						<!-- img placeholder quotes and chat bubbles  -->
						<aside class="notes">"I just want to...." Whenever people are looking for permission to do something ad hoc, in a rush, it usually means we need to slow down.</aside>
					</section>
					<section>
						<h1 class="learning">Sniff test</h1>
						<h2>Bottom-line thinking</h2>
						<!-- img placeholder quotes and chat bubbles  -->
						<aside class="notes">Focusing on costs to the exclusion of all else. Ethics & fairness not part of 'capitalism'. It's not always true, but it's deeply engrained in our psyches. Beyond your culture, people come to your company with their own history, baggage from previous jobs. Patterns are deeply engrained - sometimes we repeat them without thinking.</aside>
					</section>
					<section>
						<h2>Cultivate a culture of <u>curiosity</u></h2>
						<aside class="notes">Encourage people to follow their nose</aside>
					</section>
					<section>
						<h2>Give <u>yourself</u> permission</h2>
						<aside class="notes">Show by doing!</aside>
					</section>
					
				</section>
			</div>

		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
