<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Art vs Science</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<!-- TO DO  - highlight colors improve. Highlight important words on text only screens -->
			<div class="slides">
				<section data-background-image="img/art-vs-science.jpg">
				</section>
				<section>
					<h2>UX Research</h2>
					<p><em>in the age of the reproducibility crisis</em></p>
					<br /><br />
					<p><a class="smalllink" href="https://summerscope.github.io/slides/art-vs-science/" target="_blank">summerscope.github.io/slides/art-vs-science</a></p>
					<aside class="notes">Slides online. Don't worry about catching the URLs. I'll post the link to the slides at the end as well.</aside>
				</section>
				<section>
					<p>Our story starts with</p>
					<h2>TED</h2>
					<p><a class="smalllink" href="https://www.ted.com/talks/amy_cuddy_your_body_language_shapes_who_you_are" target="_blank">ted.com/talks/amy_cuddy_your_body_language_shapes_who_you_are</a></p>
					<aside class="notes">So back in 2012 there was a TED talk which propelled a social psychologist, Amy Cuddy, into a certain kind of fame. She was a prof at Harvard Business School. over 51 million views on TED.</aside>			
				</section>
				<section data-background-image="img/power-pose.jpg">
					<aside class="notes">She claimed that standing in a particular pose for 2 minutes a day could make you more successful. This became known as the power pose.</aside>
				</section>
				<section>
					<h2>Field study</h2>
					<aside class="notes">Her claims were based on a field study she conducted which measured self-reported feelings of power, observed risk taking + cortisol and testosterone levels from a sample of 42. Participants stood in two poses for a minute each - half in power poses, the others in non-power (weak) poses.</aside>
				</section>
				<section data-background-image="img/high-power.png">
					<p>High power poses</p>
					<aside class="notes">The two high power poses 1/2 participants held</aside>
				</section>
				<section data-background-image="img/low-power.png">
					<p>Low power poses</p>
					<aside class="notes">The two low power poses 1/2 participants held</aside>
				</section>
				<section>
					Meanwhile...
				</section>
				<section data-background-image="img/science-crisis.jpg">
					Reproducibility crisis
					<aside class="notes">In 2011 a movement started growing challenging the status quo in science. Asks whether enough work is being done to independently validate published results. Spearheaded by Brian Nosek at U of Virginia.</aside>
				</section>
				<section>
					<h2>Reproducibility project</h2>
					<aside class="notes">Around the same time  Nov 2011 - Brian Nosek of Open Science Foundation. This issue has been raised before. Feynman 1970s - Cargo Cult lecture. </aside>
				</section>
				<section>
					You'll never believe what happened when scientists attempted to replicate 100 experiments...
					<p class="fragment"><em>Just wait til you see their results</em></p>
					<aside class="notes">Psychologists tried to recreate the experiments conducted by 100 studies, all published recently in academic journals.</aside>
				</section>
				<section>
					<blockquote>
						"<b>97%</b> of the original results showed a statistically significant effect, this was reproduced in only <b>36%</b> of the replication attempts"
					</blockquote>
					<p><a class="smalllink" href="https://digest.bps.org.uk/2015/08/27/this-is-what-happened-when-psychologists-tried-to-replicate-100-previously-published-findings/" target="_blank">digest.bps.org.uk</a></p>
					<aside class="notes">To stay as true as they could the group went through extensive measures to remain true to the original studies, to the extent of consulting the original authors</aside>
				</section>
				<section>
					And then this happened...
					<aside class="notes">And guess what happened when a different researcher tried to replicate Amy Cuddy's results?√ü</aside>
				</section>
				<section data-background-image="img/research-debunked.jpg">
					<aside class="notes">A study failed to reproduce her findings on hormone changes with 4x the sample size. Her research partner Carney publically disavowed the work. Then  everyone rushed to dubunk it.</aside>
				</section>
				<section>
					<ul class="smalllink">
						<li><a href="http://fortune.com/2016/10/02/power-poses-research-false/" target="_blank">fortune.com/2016/10/02/power-poses-research-false/</a></li>
						<li><a href="http://time.com/4949675/power-poses-confidence/" target="_blank">time.com/4949675/power-poses-confidence/</a></li>
						<li><a href="https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html" target="_blank">nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html</a></li>
						<li><a href="https://www.sciencedaily.com/releases/2017/09/170911095932.htm" target="_blank">sciencedaily.com/releases/2017/09/170911095932.htm</a></li>
						<li><a href="https://journals.sagepub.com/doi/full/10.1177/0956797614553946" target="_blank">https://journals.sagepub.com/doi/full/10.1177/0956797614553946</a></li>
						<li><a href="https://www.forbes.com/sites/kimelsesser/2018/04/03/power-posing-is-back-amy-cuddy-successfully-refutes-criticism/#3c9a913c3b8e" target="_blank">forbes.com/sites/kimelsesser/2018/04/03/power-posing-is-back-amy-cuddy-successfully-refutes-criticism/#3c9a913c3b8e</a></li>
					</ul>
				</section>
				<section>
					<blockquote>"...the idea became a shorthand for flashy social psychological work that could not be&nbsp;replicated..."</blockquote>
					<p><a class="smalllink" href="https://www.tandfonline.com/doi/full/10.1080/23743603.2017.1309876" target="_blank">tandfonline.com/doi/full/10.1080/23743603.2017.1309876</a></p>
					<aside class="notes">More pop than science</aside>
				</section>
				<section>
					<p>How did studies fail to reproduce?</p>
					<ol>
						<li class="fragment">False negatives</li>
						<li class="fragment">False positives</li>
						<li class="fragment">Different effect sizes</li>
					</ol>
				</section>
				<!-- <section>
					<iframe width="746" height="420" src="https://www.youtube-nocookie.com/embed/0Rnq1NpHdmw?start=20" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				</section> -->
				<!-- <section>
					<a href="https://twitter.com/protohedgehog/status/1100079073466990593?s=21" target="_blank"><img class="plain" src="img/first-vs-right.jpg" width="500" /></a>	
				</section> -->
				<!-- <section data-background-image="img/nypost.jpg">
				</section>
				<section>
					<p><a href="https://nypost.com/2017/10/27/sniffing-your-partners-farts-could-help-ward-off-disease/" target="_blank">https://nypost.com/2017/10/27/sniffing-your-partners-farts-could-help-ward-off-disease/</a></p>
				</section> -->
				<section>
					<h2>What's to blame?</h2>
					<ul>
						<li class="fragment">"Publish or perish"</li>
						<li class="fragment">No replication studies</li>
						<li class="fragment">Clickbait</li>
					</ul>			
					<aside class="notes">Bad incentives! Scientific research in it's worst form: shitty, low-powered studies designed to be over-stated as click-bait titles and turned into scientific CV catnip</aside>
				</section>
				<section>
					<h2>P-hacking</h2>
					<img src="img/funnel_shanks.png" width="400px" />
					<p class="smalllink"><a href="http://blogs.discovermagazine.com/neuroskeptic/2015/11/10/reproducibility-crisis-the-plot-thickens/#.XKv_2Ov7TUJ" target="_blank">blogs.discovermagazine.com/neuroskeptic/2015/11/10/reproducibility-crisis-the-plot-thickens/#.XKv_2Ov7TUJ</a></p>
					<aside class="notes">Another way of expressing this would be to say that p values just below 0.05 are overrepresented. The published results ‚Äúhug‚Äù the p = 0.05 significance line. So each of the studies tended to report an effect just strong enough to be statistically significant. It‚Äôs very difficult to see how such a pattern could arise ‚Äì except through bias.</aside>
				</section>
				<section>
					<h2>Sound similar to UXR?</h2>
					<ul>
						<li class="fragment">"Prove me right"</li>
						<li class="fragment">Demanding shortcuts</li>
						<li class="fragment">Preferring 'hard data' over qual data</li>
					</ul>
					<aside class="notes">Bad incentives and a culture not set up for learning. Thinking numbers are more 'clean' or less biased than qual data.</aside>
				</section>
				<section class="references">
					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">And managers everywhere demand &quot;hard data&quot; to answer qualitative questions, even though (or because) analytics and stats are often at least as biased as qualitative observations. And it&#39;s much easier to use a number to support doing whatever you want to do anyway.</p>&mdash; Erika Hall (@mulegirl) <a href="https://twitter.com/mulegirl/status/1113228696754610177?ref_src=twsrc%5Etfw">April 2, 2019</a></blockquote>

					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Everyone is too focused on finding a magic technique or methodology instead of clarifying goals, decisions, and the larger questions you need to ask from the data (whether qualitative or quantitative) in order to make good decisions to meet your goals.</p>&mdash; Erika Hall (@mulegirl) <a href="https://twitter.com/mulegirl/status/1113229208023498752?ref_src=twsrc%5Etfw">April 2, 2019</a></blockquote>
					<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				</section>
				<!-- <section>
					<img class="plain" src="img/xkcd-new_study.png" width="300" />
					<p class="smalllink"><a href=" https://xkcd.com/1295/" target="_blank">xkcd.com/1295</a></p>
				</section> -->
				<section>
					But UXR has different goals to Science. 
					<p class="fragment">Does reproducibility even matter in UXR?</p>
					<aside class="notes">True. Maybe reproducibility isn't the goal. but it's good to know whats happening.</aside>
				</section>
				<section>
					<img class="plain" style="background:none;" src="img/crystal.png" width="300">
					<h1>Science</h1>
					<p class="fragment">Chipping away at the crystal of knowledge</p>
					<aside class="notes">Scientific research: in it's purest form, a pursuit to chip off a small shard off the crystal of knowledge</aside>
				</section>
				<section>
					<img class="plain" style="background:none;" src="img/risk.png" width="300">
					<h1>UXR</h1>
					<p class="fragment">Reducing business risk</p>
					<aside class="notes">UX research: in it's purest form, a pursuit to reduce risk and help a team intimately understand the people using their product </aside>
				</section>
				<section>
					<p>we are</p>
					<h2>Science lite</h2>
					<p class="fragment"><em>Said with ‚ù§Ô∏è</em></p>
					<aside class="notes">double diamond, design thinking, build iterate learn loop, UX research - all inspired by the scientific method. Important to see what's happening in science as that's where we draw our legitimacy from.</aside>
				</section>
				<section >
					<h2>Debunked ideas</h2>
					<div class="references">
						<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">There&#39;s a good chance that a bunch of the scientific ideas you‚Äôve learned are now outdated and debunked. Here are some of the ones I feel most strongly about üëá (1/7)</p>&mdash; Dorsa Amir (@DorsaAmir) <a href="https://twitter.com/DorsaAmir/status/1110566589601116160?ref_src=twsrc%5Etfw">March 26, 2019</a></blockquote>

						<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Are you an ENTP or an ISTJ? Turns out it doesn‚Äôt matter ¬Ø\_(„ÉÑ)_/¬Ø The Myers-Briggs personality questionnaire has pretty poor validity &amp; reliability. It&#39;s basically astrology. FYI, the &quot;Big Five&quot; is a way better personality framework. (2/7) <a href="https://t.co/3bFXN1eVpC">https://t.co/3bFXN1eVpC</a> <a href="https://t.co/7tfQrYxUb7">pic.twitter.com/7tfQrYxUb7</a></p>&mdash; Dorsa Amir (@DorsaAmir) <a href="https://twitter.com/DorsaAmir/status/1110566591077515264?ref_src=twsrc%5Etfw">March 26, 2019</a></blockquote>

						<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
					</div>
					
					<aside class="notes">Also great to keep in mind that we have lots of science 'truths' which are now debunked - don't want them to leak into our work accidentally</aside>
				</section>
				<section>
					<h2>Method/problem mismatches</h2>
					<p class="fragment">Make the hard problem of internal buy-in harder</p>
				</section>
				<section class="references">
					<!-- Avoid becoming company clickbait  -->
					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">&quot;But we did &#39;User Testing&#39;&quot;<br>Why testing prototypes won&#39;t validate your product ideas.<a href="https://t.co/AIeSNdRWfs">https://t.co/AIeSNdRWfs</a></p>&mdash; Cameron Rogers (@cameron_rogers) <a href="https://twitter.com/cameron_rogers/status/1111381603530366979?ref_src=twsrc%5Etfw">March 28, 2019</a></blockquote>

					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">It is entirely possible to do a tonne of research and keep heading in the wrong direction. It is not about volume or speed of research, it is about making sure you&#39;re looking for the right things in the right places. Please use great caution if your purpose is validation.</p>&mdash; Leisa Reichelt (@leisa) <a href="https://twitter.com/leisa/status/1111005587960029184?ref_src=twsrc%5Etfw">March 27, 2019</a></blockquote>

					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">A lot of UX <a href="https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw">#research</a> looks to me like: ‚ÄúWell we gave users two hammers and lo and behold, they pounded nails, but they pounded nails differently.‚Äù</p>&mdash; Ha Phan (@hpdailyrant) <a href="https://twitter.com/hpdailyrant/status/1113110094382678016?ref_src=twsrc%5Etfw">April 2, 2019</a></blockquote>

					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">It doesn&#39;t matter how much research you do if you haven&#39;t laid the foundation for evidence-based decision-making in advance. <br><br>Plenty of organizations pay for research, ignore it, and use that as the reason why research is a waste of resources.</p>&mdash; Erika Hall (@mulegirl) <a href="https://twitter.com/mulegirl/status/1110931323311448071?ref_src=twsrc%5Etfw">March 27, 2019</a></blockquote>

					<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

					<aside class="notes">UX Research is plagued by overgeneralisation and mismatches in tool / problem space. It's incredibly important to be specific about the things you can learn from any given research method</aside>
				</section>
				<section>
					<p>So, should <span class="highlight-green">UX Research</span> be reproducible?</p>
					<h2 class="fragment">My answer is...</h2>
				</section>
				<section>
					<h1>Sometimes!<sup class="fragment">*</sup></h1>
					<p class="fragment"><sup>*</sup>It depends</p>
					<aside class="notes">On what you mean by reproducibility</aside>
				</section>
				<section>
					At least three levels to reproducibility
				</section>
				<section>
					<h2>1. Experiment design</h2>
					<ul>
						<li class="fragment">Model &amp; Methodology</li>
						<li class="fragment">Sample size</li>
						<li class="fragment">Sample selection</li>
					</ul>
					<aside class="notes">Can I reproduce your experiment?  As in methodology, model, sample set, sample size, research variables, etc</aside>
				</section>
				<section>
					<h2>2. Data captured</h2>
					<aside class="notes">Do I get equivalent results? As in raw data </aside>
				</section>
				<section>
					<h2>3. Interpretation</h2>
					<aside class="notes">Do I interpret the data the same way? As in do I draw the same conclusions from the raw data  </aside>
				</section>
				<!-- <section>
					<blockquote>
						"I literally wake up in the middle of the night worrying about the incredible hubris of thinking that we UX'rs can do this work better than scientists. Interpreting data is hard. Identifying statistical significance is hard."
					</blockquote>
					<p><small>-Laura Summers (all the damn time)</small></p>
					<aside class="notes">Yep this is a common refrain for UX researchers - that we suffer FOMO, lack of confidence, general fear of not being scientific enough</aside>
				</section>
				<section>
					<blockquote>
						"UX research as a field suffers from a deep sense of paranoia about not appearing scientific enough."
					</blockquote>
					<p><a class="smalllink" href="https://www.fastcompany.com/3055816/from-airbnb-the-real-value-of-ux-research" target="_blank">fastcompany.com/3055816/from-airbnb-the-real-value-of-ux-research</a></small></p>
				</section> -->
				<section>
					<h2>UX Research types</h2>
					<p class="fragment">Qualitative -vs- Quantitative</p>
					<aside class="notes">Depending on the type of research, we may not even be striving for 'statistical significance'</aside>
				</section>
				<section data-background-image="img/qual-vs-quant.jpg">
				</section>
				<section>
					<h2>Qualitative</h2>
					<ol>
						<li>Experiment design <span class="fragment highlight-green">‚úî</span> </li>
						<li>Data captured <span class="fragment highlight-red">√ó</span> </li>
						<li>Interpretation <span class="fragment highlight-red">√ó</span> </li>
					</ol>
					<aside class="notes">So when we capture small data samples and deep dive qualitative data, we don't expect to reproduce the same exact results because PEOPLE </aside>
				</section>
				<section>
					<h2>Quantitative</h2>
					<ol>
						<li>Experiment design <span class="fragment highlight-green">‚úî</span></li>
						<li>Data captured <span class="fragment highlight-blue">‚úî</span></li>
						<li>Interpretation <span class="fragment highlight-blue">‚úî</span></li>
						<li class="fragment"><em>Hopefully</em> ü§û</li>
					</ol>
					<aside class="notes">But when we are doing AB tests or data driven tests into engagement, we should theoretically expect to be able to reproduce all aspects of that test </aside>
				</section>
				<!-- <section>
					Qualitative/Quantitative <br /><em>is not</em> <br />Statistically significant/Not statistically significant
					<aside class="notes">
						Note qual / quant as a binary doesn't technically fall on the does / doesn't have statistical significance binary. 
						You might technically have a qual study which had enough subjects for statistical significance but this almost never will be possible due to time and budgetary constraints.
					</aside>
				</section>
				<section>
					Sometimes you're on the cusp
					<aside class="notes">
						More importantly you might consider if you are collecting quant data but not in enough volume for statistical significance. 
					</aside>
				</section> 
				Add agenda? 
				<section>
					<h2>Learning goals</h2>
					<ol>
						<li class="fragment">What is the reproducibility crisis ‚úî</li>
						<li class="fragment">Why does it matter for UXR ‚úî </li>
						<li class="fragment">How to do it better </li>
					</ol>
					<aside class="notes"></aside>
				</section> -->
				<section>
					<h2>How do we do it better?</h2>
					<p class="fragment shrink"><em>Within the constraints of our current workplaces...</em></p>
					<aside class="notes"></aside>
				</section>
				<section>
					<h1>Any UX Research</h1>
					<aside class="notes"></aside>
				</section>
				<section>
					<p>Define your study <br /><em>before</em> <br>you start</p>
					<aside class="notes">Is it qual or quant? Are you looking for statistical significance or not? </aside>
				</section>
				<section>
					<p>Plan for no results</p>
					<aside class="notes">Regardless of your study type, sometimes the result will be that you don't know! Even with qual data sometimes you'll have lots of scattered signals but no clear signal from the noise. And just because you can't reject the null hypothesis doesn't mean you've proven it either. </aside>
				</section>
				<section>
					<h2>Sniff test</h2>
					<p>For poor research design</p>
					<aside class="notes">Who is funding the research? Are you trying to learn something or is someone trying to confirm their existing idea?</aside>
				</section>
				<section>
					<h2>Which research</h2>
					<p>is a <em>good candidate</em> for replication?</p>
				</section>
				<section class="references">
					<h2>Good</h2>
					<ul>
						<li class="fragment" data-fragment-index="1">expect the effect to be consistent over time</li>
						<li class="fragment" data-fragment-index="4">the hypothesis is important to business model functioning</li>
						<li class="fragment" data-fragment-index="6">product has been consistent since last study</li>
					</ul>				
					<h2 class="fragment" data-fragment-index="2">Poor</h2>
					<ul>
						<li class="fragment" data-fragment-index="3">don't expect the effect to be consistent</li>
						<li class="fragment" data-fragment-index="5">low risk to business model or low business priority</li>
						<li class="fragment" data-fragment-index="7">UX or UI in flux; too many variables to control for</li>
					</ul>
					<aside class="notes">Can be qual or quant. Does this effect or assumption have to remain in order for the business model to survive? Most quant testing will be hard to replicate and expect the same results- you might learn new things though. and this won't necessarily debunk the earlier results, you just won't be able to match them because of the changes that have happened.</aside>
				</section>
				<!-- <section>
					<p>Consider replication studies</p>
					<aside class="notes">do the same thing again! Even with qual data this is handy. Markets change. The zeitgeist changes. Your target users will change. Maybe a year apart, maybe with different UXrs</aside>
				</section> -->
				<section>
					<h2>Qual data</h2>
					<p>Consider trying synthesis and interpretation with multiple groups of researchers</p>
					<aside class="notes">
						- try doing UXR synthesis or usability testing interpretation by the people who didn't collect the data
						- needs experienced UXR people - swap out team mates not devs / ba.s / C-suites / other stakeholders
						- as well as the people who collected the data & compare notes and summaries / proposed next steps 
						- if your outcomes are different this isn't a disaster. You might find that there are impressions from capturing the data & conducting the research that didn't translate well to the data set. This forces you to work out how to tease out these impressions and share them with the team. 
					</aside>
				</section>
				<section data-background-image="img/affinity-mapping.jpg">
				</section>
				<section>
					<p>Preregistration</p>
					<aside class="notes">This could be an open souce and public initiative. If this is too hard a sell, even making this visible and shared within your company is a great idea.</aside>
				</section>
				<section>
					<h2>Open science movement</h2>
					<p class="fragment">What more can UXR learn/steal from the scientists?</p>
					<aside class="notes">This movement is epitomised by OSM which is pressing for scientists to share research models and design earlier (before data are captured)</aside>
				</section>
				<!-- 
				If time allows	
				<section>
					Open science movement
					<aside class="notes">This movement is epitomised by OSM which is pressing for scientists to share research models and design earlier (before data are captured)</aside>
				</section> -->
				<!-- TO DO  - breakdown into difference between all UXR and quant / data driven UXR looking for -->
				<section>
					<h1>UX Research</h1>
					<p>Attempting to identify statistically significant change</p>
					<aside class="notes"></aside>
				</section>
				<section>
					<p>Work on your research design hygiene</p>
					<aside class="notes">Identify your variables. Don't tweak them once the test has started. pre-test probability / bayesian probability is an interesting rabbit hole for those who want to skill up</aside>
				</section>
				<section>
					<h2>Is it statistically significant?</h2>
					<p class="fragment">Get better at understanding the strength of your signal</p>
					<aside class="notes"></aside>
				</section>
				<section>
					<h2>Resources</h2>
					<ul class="smalllink">
						<li><a href="https://splitly.com/statistical-calculator/" target="_blank">splitly.com/statistical-calculator</a></li>
						<li><a href="https://abtestguide.com/calc/" target="_blank">abtestguide.com/calc</a></li>
						<li><a href="https://vwo.com/ab-split-test-significance-calculator/" target="_blank">vwo.com/ab-split-test-significance-calculator/</a></li>
						<li><a href="https://neilpatel.com/ab-testing-calculator/" target="_blank">neilpatel.com/ab-testing-calculator/</a></li>
						<li><a href="https://www.surveymonkey.com/mp/ab-testing-significance-calculator/" target="_blank">surveymonkey.com/mp/ab-testing-significance-calculator/</a></li>
					</ul>
					<aside class="notes"></aside>
				</section>
				<section data-background-image="img/capture-the-stars.jpg">
					<h1>Forward</h1>
					<p class="fragment">For UXR &amp; Science</p>
				</section>
				<section>
					<blockquote>"In short, be sceptical, pick a good question, and try to answer it in many ways. It takes many numbers to get close to the truth."</blockquote>
					<p><a class="smalllink" href="https://www.nature.com/articles/d41586-019-00874-8" target="_blank">It‚Äôs time to talk about ditching statistical significance</a></p>
					<aside class="notes">To close, I think this quote summarises a great way forward for both science and UX</aside>
				</section>
				
				<section>
					<h1>Thanks!</h1>
					<p>Get the slides<br />
					<a href="https://summerscope.github.io/slides/art-vs-science/" target="_blank">summerscope.github.io/slides/art-vs-science</a></p>
				</section>
				<section>
					<h2>Further reading</h2>
					<div class="references smalllink">
						<p><a href="https://en.wikipedia.org/wiki/Reproducibility_Project" target="_blank">Wikipedia - Reproducibility Project </a></p>
						<p><a href="https://en.wikipedia.org/wiki/Null_result" target="_blank">Wikipedia - Null Result</a></p>
						<p><a href="https://digest.bps.org.uk/2014/05/20/a-replication-tour-de-force/" target="_blank">A replication tour de force</a></p>
						<p><a href="https://fivethirtyeight.com/features/psychologys-replication-crisis-has-made-the-field-better/" target="_blank">Five Thirty-eight</a></p>
						<p><a href="https://www.ncbi.nlm.nih.gov/pubmed/16060722" target="_blank">Why most published research findings are false</a></p>
						<p><a href="https://www.wired.com/2017/01/john-arnold-waging-war-on-bad-science/" target="_blank">John Arnold waging war on bad science</a></p>
						<p><a href="http://calteches.library.caltech.edu/51/2/CargoCult.htm" target="_blank">"Cargo Cult Science" by Richard Feynman</a></p>
						<p><a href="https://medium.com/mule-design/the-9-rules-of-design-research-1a273fdd1d3b" target="_blank">The 9 Rules of Design Research</a></p>
					</div>					
				</section>
				<section>
					<h2>Resources</h2>
					<div class="references smalllink">
						<p><a href="https://cos.io/" target="_blank">Center for Open Science</a></p>
						<p><a href="https://osf.io/" target="_blank">Open Science Foundation</a></p>
					</div>					
				</section>
			</div>

			<div class="twitter">
				<a href="https://twitter.com/summerscope" target="_blank">@summerscope</a>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.configure({ 
				slideNumber: true,
				slideNumber: 'c/t',
				transition: 'zoom'
			});

			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
