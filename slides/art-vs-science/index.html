<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Art vs Science</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<!-- TO DO  - highlight colors improve. Highlight important words on text only screens -->
			<div class="slides">
				<section data-background-image="img/art-vs-science.jpg">
				</section>
				<section>
					<h2>UX Research</h2>
					<p><em>in the age of the reproducibility crisis</em></p>
					<br /><br />
					<p><a class="smalllink" href="https://summerscope.github.io/slides/art-vs-science/" target="_blank">summerscope.github.io/slides/art-vs-science</a></p>
					<aside class="notes">Slides online. Don't worry about catching the URLs. I'll post the link to the slides at the end as well.</aside>
				</section>
				<section>
					<p>Our story starts with</p>
					<h2>TED</h2>
					<p><a class="smalllink" href="https://www.ted.com/talks/amy_cuddy_your_body_language_shapes_who_you_are" target="_blank">ted.com/talks/amy_cuddy_your_body_language_shapes_who_you_are</a></p>
					<aside class="notes">In 2012 Amy Cuddy, a social psychologist, gave a TED talk which propelled her to a certain kind of fame. Talk got over 51 million views on TED. (prof at Harvard Business School)</aside>			
				</section>
				<section data-background-image="img/power-pose.jpg">
					<aside class="notes">She claimed that standing in a particular pose for 2 minutes a day could make you more successful. This became known as the power pose.</aside>
				</section>
				<section>
					<h2>Field study</h2>
					<aside class="notes">Her claims were based on a field study she conducted which measured self-reported feelings of power, observed risk taking + cortisol and testosterone levels from a sample of 42. Participants stood in two poses for a minute each - half in power poses, the others in non-power (weak) poses.</aside>
				</section>
				<section data-background-image="img/high-power.png">
					<p>High power poses</p>
					<aside class="notes">Her study claimed that holding the power pose was like super power, giving you a physical and psychological edge in all kids of high stress circumstances, like negotiations and job titles. </aside>
				</section>
				<section data-background-image="img/low-power.png">
					<p>Low power poses</p>
					<aside class="notes">For a few years the power pose was catnip to corporate and boardroom types. all sorts of c-suites and execs were known to strike a pose to improve their success.</aside>
				</section>
				<section>
					Meanwhile...
				</section>
				<section data-background-image="img/science-crisis.jpg">
					Reproducibility crisis
					<aside class="notes">Meanwhile in Nov 2011 a movement kicked off challenging the status quo in science. A number of scientists were worried about the quality of results getting published, and asking whether enough work is being done to independently validate results.</aside>
				</section>
				<section>
					<h2>Reproducibility project</h2>
					<aside class="notes">Brian Nosek of Open Science Foundation U of Virginia wanted to try to reproduce published studies to see if they could replicate the effects. This issue has been raised before. Feynman 1970s - Cargo Cult lecture. </aside>
				</section>
				<section>
					You'll never believe what happened when scientists attempted to replicate 100 experiments...
					<p class="fragment"><em>Just wait til you see their results</em></p>
					<aside class="notes">Psychologists tried to recreate 100 studies, all published recently in academic journals. the group went through extensive measures to remain true to the original studies, to the extent of consulting the original authors</aside>
				</section>
				<section>
					<blockquote>
						"<b>97%</b> of the original results showed a statistically significant effect, this was reproduced in only <b>36%</b> of the replication attempts"
					</blockquote>
					<p><a class="smalllink" href="https://digest.bps.org.uk/2015/08/27/this-is-what-happened-when-psychologists-tried-to-replicate-100-previously-published-findings/" target="_blank">digest.bps.org.uk</a></p>
					<aside class="notes">Their results were pretty poor. </aside>
				</section>
				<section>
					And then this happened...
					<aside class="notes">About a year after her viral TED fame, another researcher tried to replicate Amy Cuddy's results</aside>
				</section>
				<section data-background-image="img/research-debunked.jpg">
					<aside class="notes">This study failed to reproduce her findings on hormone changes with 4x the sample size. Her research partner Carney publically disavowed the work. Then  everyone rushed to dubunk it.</aside>
				</section>
				<section>
					<blockquote>"...the idea became a shorthand for flashy social psychological work that could not be&nbsp;replicated..."</blockquote>
					<p><a class="smalllink" href="https://www.tandfonline.com/doi/full/10.1080/23743603.2017.1309876" target="_blank">tandfonline.com/doi/full/10.1080/23743603.2017.1309876</a></p>
					<aside class="notes">More pop than science</aside>
				</section>
				<!--<section>
					<p>How did studies fail to reproduce?</p>
					<ol>
						<li class="fragment">False negatives</li>
						<li class="fragment">False positives</li>
						<li class="fragment">Different effect sizes</li>
					</ol>
				</section>
				 <section>
					<iframe width="746" height="420" src="https://www.youtube-nocookie.com/embed/0Rnq1NpHdmw?start=20" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				</section> -->
				<!-- <section>
					<a href="https://twitter.com/protohedgehog/status/1100079073466990593?s=21" target="_blank"><img class="plain" src="img/first-vs-right.jpg" width="500" /></a>	
				</section> -->
				<!-- <section data-background-image="img/nypost.jpg">
				</section>
				<section>
					<p><a href="https://nypost.com/2017/10/27/sniffing-your-partners-farts-could-help-ward-off-disease/" target="_blank">https://nypost.com/2017/10/27/sniffing-your-partners-farts-could-help-ward-off-disease/</a></p>
				</section> -->
				<section>
					<h2>What's to blame?</h2>
					<ul>
						<li class="fragment">"Publish or perish"</li>
						<li class="fragment">No replication studies</li>
						<li class="fragment">Clickbait</li>
					</ul>			
					<aside class="notes">Bad incentives! Scientific research in it's worst form: shitty, low-powered studies designed to be over-stated as click-bait titles and turned into scientific CV catnip</aside>
				</section>
				<section>
					<h2>P-hacking</h2>
					<img src="img/funnel_shanks.png" width="400px" />
					<p class="smalllink"><a href="http://blogs.discovermagazine.com/neuroskeptic/2015/11/10/reproducibility-crisis-the-plot-thickens/#.XKv_2Ov7TUJ" target="_blank">blogs.discovermagazine.com/neuroskeptic/2015/11/10/reproducibility-crisis-the-plot-thickens/#.XKv_2Ov7TUJ</a></p>
					<aside class="notes">Another way of expressing this would be to say that p values just below 0.05 are overrepresented. The published results ‚Äúhug‚Äù the p = 0.05 significance line. So each of the studies tended to report an effect just strong enough to be statistically significant. It‚Äôs very difficult to see how such a pattern could arise ‚Äì except through bias.</aside>
				</section>
				<section>
					<h2>Sound similar to UXR?</h2>
					<ul>
						<li class="fragment">"Prove me right"</li>
						<li class="fragment">Demanding shortcuts</li>
						<li class="fragment">Preferring 'hard data' over qual data</li>
					</ul>
					<aside class="notes">Bad incentives and a culture not set up for learning. Thinking numbers are more 'clean' or less biased than qual data.</aside>
				</section>
				<section class="references">
					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">And managers everywhere demand &quot;hard data&quot; to answer qualitative questions, even though (or because) analytics and stats are often at least as biased as qualitative observations. And it&#39;s much easier to use a number to support doing whatever you want to do anyway.</p>&mdash; Erika Hall (@mulegirl) <a href="https://twitter.com/mulegirl/status/1113228696754610177?ref_src=twsrc%5Etfw">April 2, 2019</a></blockquote>

					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Everyone is too focused on finding a magic technique or methodology instead of clarifying goals, decisions, and the larger questions you need to ask from the data (whether qualitative or quantitative) in order to make good decisions to meet your goals.</p>&mdash; Erika Hall (@mulegirl) <a href="https://twitter.com/mulegirl/status/1113229208023498752?ref_src=twsrc%5Etfw">April 2, 2019</a></blockquote>
					<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				</section>
				<!-- <section>
					<img class="plain" src="img/xkcd-new_study.png" width="300" />
					<p class="smalllink"><a href=" https://xkcd.com/1295/" target="_blank">xkcd.com/1295</a></p>
				</section> -->
				<section>
					UXR has different goals to Science
					<aside class="notes">True. Maybe reproducibility isn't the goal. but it's good to know whats happening.</aside>
				</section>
				<section>
					<img class="plain" style="background:none;" src="img/crystal.png" width="300">
					<h1>Science</h1>
					<p class="fragment">Chipping away at the crystal of knowledge</p>
					<aside class="notes">Scientific research: in it's purest form, a pursuit to chip off a small shard off the crystal of knowledge</aside>
				</section>
				<section>
					<img class="plain" style="background:none;" src="img/risk.png" width="300">
					<h1>UXR</h1>
					<p class="fragment">Reducing business risk</p>
					<aside class="notes">UX research: in it's purest form, a pursuit to reduce risk and help a team intimately understand the people using their product </aside>
				</section>
				<section>
						<p >So does reproducibility even matter in UXR?</p>
						<p class="fragment">Well yes...</p>
				</section>
				<section>
					<p>we are</p>
					<h2>Science lite</h2>
					<small><p class="fragment"><em>Said with ‚ù§Ô∏è</em></p></small>
					<aside class="notes">double diamond, design thinking, build iterate learn loop, UX research - all inspired by the scientific method. Important to see what's happening in science as that's where we draw our legitimacy from.</aside>
				</section>
				<section >
					<h2>Debunked ideas</h2>
					<div class="references">
						<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">There&#39;s a good chance that a bunch of the scientific ideas you‚Äôve learned are now outdated and debunked. Here are some of the ones I feel most strongly about üëá (1/7)</p>&mdash; Dorsa Amir (@DorsaAmir) <a href="https://twitter.com/DorsaAmir/status/1110566589601116160?ref_src=twsrc%5Etfw">March 26, 2019</a></blockquote>

						<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Are you an ENTP or an ISTJ? Turns out it doesn‚Äôt matter ¬Ø\_(„ÉÑ)_/¬Ø The Myers-Briggs personality questionnaire has pretty poor validity &amp; reliability. It&#39;s basically astrology. FYI, the &quot;Big Five&quot; is a way better personality framework. (2/7) <a href="https://t.co/3bFXN1eVpC">https://t.co/3bFXN1eVpC</a> <a href="https://t.co/7tfQrYxUb7">pic.twitter.com/7tfQrYxUb7</a></p>&mdash; Dorsa Amir (@DorsaAmir) <a href="https://twitter.com/DorsaAmir/status/1110566591077515264?ref_src=twsrc%5Etfw">March 26, 2019</a></blockquote>

						<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
					</div>
					
					<aside class="notes">Also great to keep in mind that we have lots of science 'truths' which are now debunked - don't want them to leak into our work accidentally</aside>
				</section>
				<section>
					<h2>Method/problem mismatches</h2>
					<p class="fragment">Make the hard problem of internal buy-in harder</p>
				</section>
				<section class="references">
					<!-- Avoid becoming company clickbait  -->
					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">&quot;But we did &#39;User Testing&#39;&quot;<br>Why testing prototypes won&#39;t validate your product ideas.<a href="https://t.co/AIeSNdRWfs">https://t.co/AIeSNdRWfs</a></p>&mdash; Cameron Rogers (@cameron_rogers) <a href="https://twitter.com/cameron_rogers/status/1111381603530366979?ref_src=twsrc%5Etfw">March 28, 2019</a></blockquote>

					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">It is entirely possible to do a tonne of research and keep heading in the wrong direction. It is not about volume or speed of research, it is about making sure you&#39;re looking for the right things in the right places. Please use great caution if your purpose is validation.</p>&mdash; Leisa Reichelt (@leisa) <a href="https://twitter.com/leisa/status/1111005587960029184?ref_src=twsrc%5Etfw">March 27, 2019</a></blockquote>

					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">A lot of UX <a href="https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw">#research</a> looks to me like: ‚ÄúWell we gave users two hammers and lo and behold, they pounded nails, but they pounded nails differently.‚Äù</p>&mdash; Ha Phan (@hpdailyrant) <a href="https://twitter.com/hpdailyrant/status/1113110094382678016?ref_src=twsrc%5Etfw">April 2, 2019</a></blockquote>

					<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">It doesn&#39;t matter how much research you do if you haven&#39;t laid the foundation for evidence-based decision-making in advance. <br><br>Plenty of organizations pay for research, ignore it, and use that as the reason why research is a waste of resources.</p>&mdash; Erika Hall (@mulegirl) <a href="https://twitter.com/mulegirl/status/1110931323311448071?ref_src=twsrc%5Etfw">March 27, 2019</a></blockquote>

					<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

					<aside class="notes">UX Research is plagued by overgeneralisation and mismatches in tool / problem space. It's incredibly important to be specific about the things you can learn from any given research method</aside>
				</section>
				<section>
					<p>So, should <span class="highlight-green">UX Research</span> be reproducible?</p>
					<h2 class="fragment">My answer is...</h2>
				</section>
				<section>
					<h1>Sometimes!<sup class="fragment">*</sup></h1>
					<p class="fragment"><sup>*</sup>It depends</p>
					<aside class="notes">It depends on what you mean by reproducibility</aside>
				</section>
				<section>
					At least three levels to reproducibility
				</section>
				<section>
					<h2>1. Experiment design</h2>
					<ul>
						<li class="fragment">Model &amp; Methodology</li>
						<li class="fragment">Sample size</li>
						<li class="fragment">Sample selection</li>
					</ul>
					<aside class="notes">Can I reproduce your experiment?  As in methodology, model, sample set, sample size, research variables, etc</aside>
				</section>
				<section>
					<h2>2. Data captured</h2>
					<aside class="notes">Do I get equivalent results? As in raw data </aside>
				</section>
				<section>
					<h2>3. Interpretation</h2>
					<aside class="notes">Do I interpret the data the same way? As in do I draw the same conclusions from the raw data  </aside>
				</section>
				<!--
				<section>
					<blockquote>
						"I literally wake up in the middle of the night worrying about the incredible hubris of thinking that we UX'rs can do this work better than scientists. Interpreting data is hard. Identifying statistical significance is hard."
					</blockquote>
					<p><small>-Laura Summers (all the damn time)</small></p>
					<aside class="notes">Yep this is a common refrain for UX researchers - that we suffer FOMO, lack of confidence, general fear of not being scientific enough</aside>
				</section>
				<section>
					<blockquote>
						"UX research as a field suffers from a deep sense of paranoia about not appearing scientific enough."
					</blockquote>
					<p><a class="smalllink" href="https://www.fastcompany.com/3055816/from-airbnb-the-real-value-of-ux-research" target="_blank">fastcompany.com/3055816/from-airbnb-the-real-value-of-ux-research</a></small></p>
				</section> 
				-->
				<section>
					<h2>UX Research types</h2>
					<p class="fragment">Qualitative -vs- Quantitative</p>
					<aside class="notes">Depending on the type of research, we may not even be striving for 'statistical significance'</aside>
				</section>
				<section data-background-image="img/qual-vs-quant.jpg">
					<aside class="notes">With Qual at least, we aren't striving for statistical significance and the question of reproducibility goes out the window.</aside>
				</section>
				<section>
					<h2>Qualitative</h2>
					<ol>
						<li>Experiment design <span class="fragment highlight-green">‚úî</span> </li>
						<li>Data captured <span class="fragment highlight-red">√ó</span> </li>
						<li>Interpretation <span class="fragment highlight-red">√ó</span> </li>
					</ol>
					<aside class="notes">So when we capture small data samples and deep dive qualitative data, we don't expect to reproduce the same exact results because PEOPLE </aside>
				</section>
				<section>
					<h2>Quantitative</h2>
					<ol>
						<li>Experiment design <span class="fragment highlight-green">‚úî</span></li>
						<li>Data captured <span class="fragment highlight-blue">‚úî</span></li>
						<li>Interpretation <span class="fragment highlight-blue">‚úî</span></li>
						<li class="fragment"><em>Hopefully</em> ü§û</li>
					</ol>
					<aside class="notes">But when we are doing AB tests or data driven tests into engagement, we should theoretically expect to be able to reproduce all aspects of that test </aside>
				</section>
				<!-- <section>
					Qualitative/Quantitative <br /><em>is not</em> <br />Statistically significant/Not statistically significant
					<aside class="notes">
						Note qual / quant as a binary doesn't technically fall on the does / doesn't have statistical significance binary. 
						You might technically have a qual study which had enough subjects for statistical significance but this almost never will be possible due to time and budgetary constraints.
					</aside>
				</section>
				<section>
					Sometimes you're on the cusp
					<aside class="notes">
						More importantly you might consider if you are collecting quant data but not in enough volume for statistical significance. 
					</aside>
				</section> 
				Add agenda? 
				<section>
					<h2>Learning goals</h2>
					<ol>
						<li class="fragment">What is the reproducibility crisis ‚úî</li>
						<li class="fragment">Why does it matter for UXR ‚úî </li>
						<li class="fragment">How to do it better </li>
					</ol>
					<aside class="notes"></aside>
				</section> -->
				<section>
					<h2>How do we do it better?</h2>
					<p class="fragment grow"><em>Get inspired by the Open Science movement</em></p>
					<p class="fragment shrink"><em>Within the constraints of our current workplaces...</em></p>
					<aside class="notes"></aside>
				</section>
				<section>
					<h1>Crunching numbers</h1>
					<p class="fragment">So you want statistical significance</p>
					<aside class="notes">Spend some time booting up</aside>
				</section>
				<section>
					<ul>
						<li>Work on your research  <span class="fragment highlight-green">design hygiene</span></li>
						<li>Get better at understanding the <span class="fragment highlight-green">strength of your signal</span></li>
					</ul>
					<aside class="notes">Identify your variables. Don't tweak them once the test has started. pre-test probability / bayesian probability is an interesting rabbit hole for those who want to skill up</aside>
				</section>
				<section data-background-image="img/ab-test.png">
				</section>
				<!-- BREAK -->
	
				<section>
					<img src="img/research-methods.png" width="600" />
					<p class="smalllink"><a href="https://www.nngroup.com/articles/which-ux-research-methods/" target="_blank">nngroup.com/articles/which-ux-research-methods/</a></p>
					<aside class="notes">Reminder: we have this big world of research methods! We don't have to have statistical significance to learn something, or to decide what to try next. And thinking about getting better at research will help us navigate with more agility.</aside>
				</section>
				<section>
					<h2>Embrace uncertainty</h2>
					<p class="fragment"><em>Like a scientist</em></p>
					<ul>
						<li class="fragment">Avoid speaking in absolutes</li>
						<li class="fragment">Cultivate your curiousity</li>
						<li class="fragment">Reward <em>"I don't know"</em></li>
					</ul>
					<aside class="notes">Science is Moving towards certainty, but never 100% there.</aside>
				</section>
				<section>
					<img class="plain" src="img/research-design.png" width="300px" style="background:none; float:left;" />
					<br /><br />
					<p>Define your study <br /><em>before</em> <br>you start</p>
					<aside class="notes">"Preregistration". Is it qual or quant? Are you looking for statistical significance or not?</aside>
				</section>
				<section>
					<p>Plan for no conclusion <br><em>(unclear results)</em></p>
					<aside class="notes">a null result is a result without the expected content: that is, the proposed result is absent. Regardless of your study type, sometimes the result will be that you don't know! Even with qual data sometimes you'll have lots of scattered signals but no clear signal from the noise. And just because you can't reject the null hypothesis doesn't mean you've proven it either.</aside>
				</section>
				<!-- <section>
					<h2>Sniff test</h2>
					<p>For poor research design</p>
					<aside class="notes">Who is funding the research? Are you trying to learn something or is someone trying to confirm their existing idea?</aside>
				</section> -->
				<section>
					<h2>Replication studies</h2>
					<p>Start by identifying a <em>good candidate</em> study</p>
					<aside class="notes">So replication studies - doing the same study again - helps us be more confident in the results. </aside>
				</section>
				<section class="references">
					<h2>Good</h2>
					<ul>
						<li class="fragment" data-fragment-index="1">expect the effect to be consistent over time</li>
						<li class="fragment" data-fragment-index="4">the hypothesis is important to business model functioning</li>
						<li class="fragment" data-fragment-index="6">product has been consistent since last study</li>
					</ul>				
					<h2 class="fragment" data-fragment-index="2">Poor</h2>
					<ul>
						<li class="fragment" data-fragment-index="3">don't expect the effect to be consistent</li>
						<li class="fragment" data-fragment-index="5">low risk to business model or low business priority</li>
						<li class="fragment" data-fragment-index="7">UX or UI in flux; too many variables to control for</li>
					</ul>
					<aside class="notes">Can be qual or quant. Does this effect or assumption have to remain in order for the business model to survive? Most quant testing will be hard to replicate and expect the same results- you might learn new things though. and this won't necessarily debunk the earlier results, you just won't be able to match them because of the changes that have happened.</aside>
				</section>
				<!-- <section>
					<p>Consider replication studies</p>
					<aside class="notes">do the same thing again! Even with qual data this is handy. Markets change. The zeitgeist changes. Your target users will change. Maybe a year apart, maybe with different UXrs</aside>
				</section> -->
				<section>
					<h2>Qual data</h2>
					<p>Consider trying synthesis and interpretation with multiple groups of researchers</p>
					<aside class="notes">
						- interpretation by the people who didn't collect the data
						- needs experienced UXR people 
						- compare summaries / proposed next steps 
					</aside>
				</section>
				<section data-background-image="img/affinity-mapping.jpg">
				</section>
				<section>
					A mismatch <br />could be an opportunity to learn
					<aside class="notes"> if your outcomes are different this isn't a disaster. You might find that there are impressions from capturing the data & conducting the research that didn't translate well to the data set. This forces you to work out how to tease out these impressions and share them with the team. OR: might help you catch cognitive bias which resulted from conducting the resarch. </aside>
				</section>
				<section data-background-image="img/power-ups.jpg">
				</section>
				<section>
					<h2>Fact check</h2>
					<ul>
						<li class="fragment">Learn to read references</li>
						<li class="fragment">Paper abstract is a good start</li>
						<li class="fragment">Develop your sniff test</li>
					</ul>
					<aside class="notes">When you're reading or watching the news, actually look for and check the science citation! Learn to read scientific papers. Tell story about + 40% GDPR - adelaide uni ML brochure - if time allows.</aside>
				</section>
				<section>
					<h2>Open UX<span class="fragment">...?</span></h2>
					<aside class="notes">* open data * open results * open methods * share research templates * FOSS style? Make skill guilds across businesses? Peer review each other's research? Do meta studies? Come talk to me after and let's discuss. </aside>
				</section>
				<section>
					<h2>Continuous discovery<span class="fragment">...?</span></h2>
					<aside class="notes">visualising what we know over time. change points. discovery points. a resource for the entire team and a different way of thinking about building a body of knowledge.  </aside>
				</section>

				<!-- 
				<section>
					<p>Preregistration</p>
					<aside class="notes">This could be an open souce and public initiative. If this is too hard a sell, even making this visible and shared within your company is a great idea.</aside>
				</section>
				<section>
					<h2>Open science movement</h2>
					<p class="fragment">What more can UXR learn/steal from the scientists?</p>
					<aside class="notes">This movement is epitomised by OSM which is pressing for scientists to share research models and design earlier (before data are captured)</aside>
				</section>
				
				If time allows	
				<section>
					Open science movement
					<aside class="notes">This movement is epitomised by OSM which is pressing for scientists to share research models and design earlier (before data are captured)</aside>
				</section> -->
				<!-- TO DO  - breakdown into difference between all UXR and quant / data driven UXR looking for -->
				
				<!-- <section>
					<h2>Is it statistically significant?</h2>
					<p class="fragment">Get better at understanding the strength of your signal</p>
					<aside class="notes"></aside>
				</section> -->
				<section>
					<h2>Recommendations</h2>
					<ul>
						<li>Build culture for continuous learning</li>
						<li>Embrace uncertainty</li>
						<li>Replication studies (maybe)</li>
						<li>Spend more time on research design</li>
						<li>Uncertain results are ok</li>
					</ul>
				</section>
				<section data-background-image="img/capture-the-stars.jpg">
					<h1>Forward</h1>
					<p class="fragment">For UXR &amp; Science</p>
				</section>
				<section>
					<blockquote>"In short, be sceptical, pick a good question, and try to answer it in many ways. It takes many numbers to get close to the truth."</blockquote>
					<p><a class="smalllink" href="https://www.nature.com/articles/d41586-019-00874-8" target="_blank">It‚Äôs time to talk about ditching statistical significance</a></p>
					<aside class="notes">To close, I think this quote summarises a great way forward for both science and UX</aside>
				</section>
				<section>
					<h1>Thanks!</h1>
					<p>Get the slides<br />
					<a href="https://summerscope.github.io/slides/art-vs-science/" target="_blank">summerscope.github.io/slides/art-vs-science</a></p>
				</section>
				<section>
					<h2>Power Pose</h2>
					<ul class="smalllink">
						<li><a href="http://fortune.com/2016/10/02/power-poses-research-false/" target="_blank">fortune.com/2016/10/02/power-poses-research-false/</a></li>
						<li><a href="http://time.com/4949675/power-poses-confidence/" target="_blank">time.com/4949675/power-poses-confidence/</a></li>
						<li><a href="https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html" target="_blank">nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html</a></li>
						<li><a href="https://www.sciencedaily.com/releases/2017/09/170911095932.htm" target="_blank">sciencedaily.com/releases/2017/09/170911095932.htm</a></li>
						<li><a href="https://journals.sagepub.com/doi/full/10.1177/0956797614553946" target="_blank">https://journals.sagepub.com/doi/full/10.1177/0956797614553946</a></li>
						<li><a href="https://www.forbes.com/sites/kimelsesser/2018/04/03/power-posing-is-back-amy-cuddy-successfully-refutes-criticism/#3c9a913c3b8e" target="_blank">forbes.com/sites/kimelsesser/2018/04/03/power-posing-is-back-amy-cuddy-successfully-refutes-criticism/#3c9a913c3b8e</a></li>
					</ul>
				</section>
				<section>
					<h2>A/B Tools</h2>
					<ul class="smalllink">
						<li><a href="https://splitly.com/statistical-calculator/" target="_blank">splitly.com/statistical-calculator</a></li>
						<li><a href="https://abtestguide.com/calc/" target="_blank">abtestguide.com/calc</a></li>
						<li><a href="https://vwo.com/ab-split-test-significance-calculator/" target="_blank">vwo.com/ab-split-test-significance-calculator/</a></li>
						<li><a href="https://neilpatel.com/ab-testing-calculator/" target="_blank">neilpatel.com/ab-testing-calculator/</a></li>
						<li><a href="https://www.surveymonkey.com/mp/ab-testing-significance-calculator/" target="_blank">surveymonkey.com/mp/ab-testing-significance-calculator/</a></li>
					</ul>
					<aside class="notes"></aside>
				</section>
				<section>
					<h2>Further reading</h2>
						<ul class="references smalllink">
							<li><a href="https://youtu.be/0Rnq1NpHdmw" target="_blank">Scientific Studies: Last Week Tonight with John Oliver</a></li>
							<li><a href="https://en.wikipedia.org/wiki/Reproducibility_Project" target="_blank">Wikipedia - Reproducibility Project </a></li>
							<li><a href="https://en.wikipedia.org/wiki/Null_result" target="_blank">Wikipedia - Null Result</a></li>
							<li><a href="https://digest.bps.org.uk/2014/05/20/a-replication-tour-de-force/" target="_blank">A replication tour de force</a></li>
							<li><a href="https://fivethirtyeight.com/features/psychologys-replication-crisis-has-made-the-field-better/" target="_blank">Five Thirty-eight</a></li>
							<li><a href="https://www.ncbi.nlm.nih.gov/pubmed/16060722" target="_blank">Why most published research findings are false</a></li>
							<li><a href="https://www.wired.com/2017/01/john-arnold-waging-war-on-bad-science/" target="_blank">John Arnold waging war on bad science</a></li>
							<li><a href="http://calteches.library.caltech.edu/51/2/CargoCult.htm" target="_blank">"Cargo Cult Science" by Richard Feynman</a></li>
							<li><a href="https://medium.com/mule-design/the-9-rules-of-design-research-1a273fdd1d3b" target="_blank">The 9 Rules of Design Research</a></li>
							<li><a href="https://www.technologyreview.com/the-download/612982/machine-learning-is-contributing-to-a-reproducibility-crisis-within-science/" target="_blank">ML contributing to a reproducibility crisis within science</a></li>
							<li><a href="https://deardesignstudent.com/the-secret-cost-of-research-fbe95739afdd" target="_blank">The secret cost of research</a></li>
							<li><a href="https://www.vox.com/science-and-health/2018/8/27/17761466/psychology-replication-crisis-nature-social-science" target="_blank">More social science studies just failed to replicate</a></li>							
							<li><a href="https://cos.io/" target="_blank">Center for Open Science</a></li>
							<li><a href="https://osf.io/" target="_blank">Open Science Foundation</a></li>
						</ul>
				</section>
			</div>

			<div class="twitter">
				<a href="https://twitter.com/summerscope" target="_blank">@summerscope</a>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.configure({ 
				slideNumber: true,
				slideNumber: 'c/t',
				transition: 'zoom'
			});

			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
