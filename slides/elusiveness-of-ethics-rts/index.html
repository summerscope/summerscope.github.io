<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>The elusiveness of ethics: encoding fairness in an unfair world</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-background-image="img/Tactile_3.jpg">
					<h1>The elusiveness of ethics</h1> 
					<h3>Encoding fairness in an unfair world</h3>
				</section>				
				<section>
					<h2>About me</h2>
					<table>
						<tr>
							<td align="center">
								<img class="plain" src="img/DEBIAS-AI.svg" alt="Debias AI" width="280px" />
							</td>
							<td align="center">
								<img class="plain" src="img/ethics-litmus-tests.png" height="280px" alt="Ehics Litmus Tests" />
							</td>
						</tr>
						<tr>
							<td align="center">
								<small><a href="https://debias.ai" target="_blank">debias.ai</a></small>
							</td>
							<td align="center">
								<small><a href="https://ethical-litmus.site" target="_blank">ethical-litmus.site</a></small>
							</td>
						</tr>
					</table>
					<aside class="notes">About me -  Ethics Litmus Tests (preorder live), background, learning deep learning & ML, reading papers, Fair ML reading group</aside>
				</section>
				<section>
					<h2>Slides</h2>
					<p><small><a href="http://summerscope.github.io/slides/elusiveness-of-ethics-rts" target="_blank">summerscope.github.io/slides/elusiveness-of-ethics-rts</a></small></p>
					<aside class="notes">Links all live, don't stress about trying to write down URLs</aside>
				</section>
				<section>
					<section>
						<h1>Section 1</h1>
					</section>
					<section>
						<h2>Desanctifying the charisma of numbers</h2>
						<p><small><a href="https://www.tandfonline.com/doi/full/10.1080/17530350.2018.1527710" target="_blank">https://www.tandfonline.com/doi/full/10.1080/17530350.2018.1527710</a></small></p>
						<aside class="notes">Paper comparing Calculated Values by William Deringer and The Tyranny of Metrics by Jerry Z. Muller - both works exploring metric fixation, collective fascination with calculation. At the end of the discussion we considered that there are at least two distinct kinds of metric problems: problems of calculation, and problems of framing.</aside>
					</section>
					<section>
						<h2>Descriptive <br> -vs- <br> Normative</h2>
						<aside class="notes">Following on from and perhaps compounding that, I'd argue that we often mix up whether our metrics / data science acts with a descriptive or normative force.</aside>
					</section>
					<section data-background-image="img/mean-girls.jpg">
						<h2>Girls wear pink</h2>
						<h3 class="fragment">(Descriptive)</h3>
					</section>
					<section data-background-image="img/sabrina.gif">
						<h2>Girls wear pink</h2>
						<h3 class="fragment">(Normative)</h3>
					</section>
					<section>
						<h2>The way the world is?</h2>
						<h2 class="fragment">or</h2>
						<h2 class="fragment">The way the world should&nbsp;be?</h2>
					</section>
					<section>
						<h2>For example...</h2>
					</section>
					<section data-background-image="img/drone.jpg">
						<aside class="notes">Drone assessing a bridge. It's observing the state of the world (damage to bridge) and the system makes a normative claim: we want to identify and repair damage to bridges! Cracks in bridges are observed in the world (due to various forces of attrition) but not accepted as the inevitable state.</aside>
					</section>
					<section>
						<h2>We make normative assertions all the time</h2>
						<aside class="notes">And it's normal, not interpreted as moralising or judgemental. But we can get squeamish about doing this when it comes to tech ethics. More on this soon.</aside>
					</section>
					<section>
						<h2>Think of the drone...</h2>
						<aside class="notes">If you ever get pushback on making a call like this, think of the drone. Someone designed a rule into the system which identified what was wrong that needed to be repaired. The judgement call is implied. It's the 'status quo' is not good rationale for non-intervention.</aside>
					</section>
					<section>
						<iframe width="560" height="315" src="https://www.youtube.com/embed/uHGlCi9jOWY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
						<p><small><a href="https://www.youtube.com/watch?v=uHGlCi9jOWY" target="_blank">https://www.youtube.com/watch?v=uHGlCi9jOWY</a></small></p>
					</section>
					<section>
						<h2>Profit vs. Knowledge</h2>
						<p class="fragment">-or-</p>
						<h2 class="fragment">Winning is not the same as&nbsp;understanding</h2>
						<aside class="notes">The work to build and train an ML model to 'win' is not the same as the work to understand the relationship between the data/model structure to the outside world</aside>
					</section>
					<section>
						<h2>Can a mousetrap be... unethical?</h2>
						<aside class="notes">Should you care, as long as it catches more mice (than the others)? how do you know if your mouse trap catches more of a specific gender, age or species of mouse? Is it an unfair mousetrap? Does it matter?</aside>
					</section>
					<section>
						<p><em>Building ML systems in a capitalist, corporate context:</em></p>
						<aside class="notes">Here's my pitch - for our current use of models</aside>
					</section>
					<section>
						<h2>There is no prediction; only intervention</h2>
						<aside class="notes">Why build a model to predict if you're not going to do anything about it? All prediction has some intended intervention.</aside>
					</section>
					<section>
						<h2>Classification is never descriptive, always normative</h2>
						<aside class="notes">The boy and the bundle of sticks? Everything we tell people is a story they learn about themselves. Even if they reject the story, it becomes ingraned in their identity.</aside>
					</section>
					<section>
						<h2>So the moral is...</h2>
						<p class="fragment">Predictive models are almost never <em>descriptive</em> when deployed within a capitalist framework</p>
						<aside class="notes">Even when we think we're building a mouse trap, that our tech is neutral, we are almost certainly intervening in the world, exerting a normative force, or both.</aside>
					</section>
				</section>
				<section>
					<section>
						<h1>Section 2</h1>
						<h3 class="fragment">Taxonomies of Bias</h3>
						<aside class="notes"></aside>
					</section>
					<section>
						<h2>Drawn from this paper</h2>
						<p>A Framework for Understanding Unintended Consequences of Machine Learning</p>
						<p><a href="https://arxiv.org/pdf/1901.10002.pdf" target="_blank">https://arxiv.org/pdf/1901.10002.pdf</a></p>
					</section>
					<section>
						<img class="plain" src="img/types-of-harm.png" height="180" />
						<h2>Two primary types of harm</h2>
						<ol>
							<li class="fragment">Allocative harms</li>
							<li class="fragment">Representational harms</li>
						</ol>
					</section>
					<section>
						<h2>A map to plot bias</h2>
					</section>
					<section data-background-image="img/taxonomy.jpg" data-transition="none">
						<aside class="notes">I've seeen a number of different maps but they are often confusing, too many steps to keep in your head, aiming for something easier to grok / keep in your head.</aside>			
					</section>
					<section data-background-image="img/taxonomy1.jpg">
						<aside class="notes">So we'll think through the types of bias we might see at each stage, and approaches to mitigation for each type. We'll start with the world we gather data from...</aside>			
					</section>
					<section>
						<img class="plain" src="img/taxonomy-data-source.png" height="200" />
						<h3 class="fragment"> Is this good training data?</h3>
						<p class="fragment">✅ Historical Bias</p>
						<aside class="notes">Not all data represents good decision making, or patterns we want to repeat. We want to look at the thing we're trying to teach the machine and ask if we want to learn from this or not.</aside>
					</section>
					<section>
						<p>✅ Historical Bias</p>
						<blockquote class="fragment">Normative statement: a misalignment between the world as it is, and your values or objectives</blockquote>
						<aside class="notes">Closely related to structural inequality & intersectional bias. Note that Historical Bias is an amplifier of almost all the other problems we'll discuss. </aside>
					</section>
					<section>
						<p>✅ Historical Bias examples</p>
						<ul>
							<li class="fragment"><a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G" target="_blank">Amazon recruitment algorithm</a></li>
							<li class="fragment">Google image search: Professional hair / CEO</li>
						</ul>
						<aside class="notes">Closely related to structural inequality & intersectional bias. Note that Historical Bias is an amplifier of almost all the other problems we'll discuss. </aside>
					</section>
					<section>
						<p>✅ Mitigating Historical Bias</p>
						<ul>
							<li class="fragment">Can't manage what we can't measure</li>
							<li class="fragment">Test for model accuracy &amp; allocation across sub-demographics</li>
							<li class="fragment">Encode 'positive bias' - think affirmative action, for models</li>
							<li class="fragment"><a href="https://arxiv.org/abs/1703.06856v3" target="_blank">Counterfactual fairness</a></li>
						</ul>
						<aside class="notes">Political policy, deciding /not/ to do the model. Grappling with historical bias requires expertise outside of dev - sociology, poli sci, intersectional feminist theory</aside>
					</section>
					<section data-background-image="img/taxonomy2.jpg">
						<aside class="notes"></aside>			
					</section>
					<section>
						<img class="plain" src="img/taxonomy-data-capture.png" height="200" />
						<h3 class="fragment">Can we capture this data well?</h3>
						<p class="fragment">✅ Measurement / Sampling Bias</p>
						<p class="fragment">✅ Representation Bias</p>						
						<aside class="notes">Does enough data exist in-theory across all the demographics? Data capture techniques? Data storage constraints?</aside>
					</section>
					<section>
						<p>✅ Measurement Bias</p>
						<blockquote class="fragment">Problems with choosing, collection, or computing features and labels to use in a prediction problem</blockquote>
						<aside class="notes">Noisy proxies. Sampling methods.</aside>
					</section>
					<section>
						<p>✅ Measurement Bias</p>
						<ul>
							<li class="fragment">Measurement process varies across groups</li>
							<li class="fragment">Quality of data varies across groups</li>
							<li class="fragment">Defined classification task is an oversimplification</li>
						</ul>
						<aside class="notes">Noisy proxies. Sampling methods.</aside>
					</section>
					<section>
						<p>✅ Measurement Bias examples</p>
						<ul>
							<li class="fragment"><a href="https://www.theverge.com/2019/11/11/20958953/apple-credit-card-gender-discrimination-algorithms-black-box-investigation" target="_blank">Apple credit card scandal</a></li>
							<li class="fragment"><a href="http://www.predpol.com/" target="_blank">PredPol - predictive policing</a></li>
						</ul>
						<aside class="notes">Over-measuring crime in poor/black neighborhoods ensures future over-policing (if more crimes by POC are measured and reported than caucasians)</aside>
					</section>
					<section>
						<p>✅ Mitigating Measurement Bias</p>
						<ul>
							<li class="fragment">Review sampling methodologies</li>
							<li class="fragment">Assess proxy quality</li>
							<li class="fragment">Synthetic data</li>
						</ul>
						<aside class="notes">Capturing and labelling data. Better sampling techniques / Synthetic Data to flesh out poorly represented demographics. Looking for better proxies for the quality you want to measure. Apple CC example if time allows </aside>
					</section>
					<section data-background-image="img/taxonomy3.jpg">
						<aside class="notes"></aside>							
					</section>
					<section>
						<img class="plain" src="img/taxonomy-lab.png" height="200" />
						<h3 class="fragment">Can we form useful inferences from our&nbsp;data?</h3>
						<p class="fragment">✅ Aggregation Bias</p>
						<p class="fragment">✅ Evaluation Bias</p>
						<aside class="notes"></aside>
					</section>
					<section>
						<p>✅ Aggregation Bias</p>
						<blockquote class="fragment">When a one-size-fit-all model is used for groups with different conditional distributions</blockquote>
						<aside class="notes"></aside>
					</section>
					<section>
						<p>✅ Aggregation Bias examples</p>
						<ul>
							<li class="fragment">Everything in <a href="https://www.booktopia.com.au/invisible-women-caroline-criado-perez/book/9781784742928.html" target="_blank">Invisible Women</a> by Caroline&nbsp;Criado&nbsp;Perez</li>
							<li class="fragment"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2757062/" target="_blank">Skin cancer</a> <a href="https://www.cnet.com/health/how-to-use-your-smartphone-to-detect-skin-cancer/" target="_blank">scanning apps</a></li>
						</ul>
						<aside class="notes">Any time you fail to detect important variations in groups due to lumping them together</aside>
					</section>
					<section>
						<p>✅ Mitigating Aggregation Bias</p>
						<ul>
							<li class="fragment">Consider salient differences in sub-demographics for your model</li>
							<li class="fragment">Add more detailed classifications</li>
							<li class="fragment">Identify <em>for whom</em> your model performs well</li>
						</ul>
						<aside class="notes"></aside>
					</section>
					<section>
						<p>✅ Evaluation Bias</p>
						<blockquote class="fragment">Occurs when the evaluation and/or benchark data for an algorithm doesn’t represent the target population</blockquote>
						<aside class="notes">A misrepresentative benchmark encourages the development of models that only perform well on a subset of the population.</aside>
					</section>
					<section>
						<p>✅ Evaluation Bias examples</p>
						<ul>
							<li class="fragment"><a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf" target="_blank">Benchmark datasets</a> with insufficient representational data</li>
						</ul>
						<aside class="notes">Facial recognition is having a moment - this is largely due to this work by Joy Buolamwini & Timnit Gebru </aside>
					</section>
					<section>
						<p>✅ Mitigating Evaluation Bias</p>
						<ul>
							<li class="fragment">More representative benchmark datasets </li>
							<li class="fragment">More granular benchmarks (sub-group evaluation)</li>
							<li class="fragment">Multiple confidence levels</li>
						</ul>
						<aside class="notes">ML design (modeling decisions). We might deploy an explainability model alongside our ML model to better understand the features that it learns from and identify bias. We can put our thumb on the scale - adjust classifications or scores to inject positive bias. </aside>
					</section>
					<section data-background-image="img/taxonomy4.jpg">
						<aside class="notes"></aside>							
					</section>
					<section>
						<img class="plain" src="img/taxonomy-factory.png" height="200" />
						<h3 class="fragment">Can we deploy those inferences effectively?</h3>
						<p class="fragment">✅ Deployment Bias</p>
						<aside class="notes">Identify & adjust bias retroactively after classification. Externalising machine uncertainty. Adding design thinking into the deployment of ML - whether a full automation, has a hand-off, or a decision aid.</aside>
					</section>	
					<section>
						<p>✅ Deployment Bias</p>
						<blockquote class="fragment">A mismatch between the problem a model is intended to solve and the way in which it is actually used.</blockquote>
						<aside class="notes"></aside>
					</section>
					<section>
						<p>✅ Deployment Bias examples</p>
						<ul>
							<li class="fragment"><a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank">Recidivism risk scores</a></li>
							<li class="fragment"><a href="https://www.youtube.com/watch?v=WXuK6gekU1Y&feature=youtu.be&gclid=Cj0KCQjwuJz3BRDTARIsAMg-HxUTfaxmysOkbLZXDsRnKhVIlQS0Qm79VHrv-b2ETVTyDWF4eczm9WwaAliNEALw_wcB" target="_blank">AlphaGo - anthropomorphising the machine</a></li>
						</ul>
						<aside class="notes"></aside>
					</section>
					<section>
						<p>✅ Mitigating Deployment Bias</p>
						<ul>
							<li class="fragment">Involve designers, product managers, security people, and other product disciplines in application design</li>
							<li class="fragment">Conduct UX research on the use of your tool in the real world</li>
							<li class="fragment">Externalise model uncertainty</li>
							<li class="fragment">Design failure &amp; recourse flows (first)</li>
						</ul>
						<aside class="notes"></aside>
					</section>		
					<section>
						<h2>Further reading</h2>
						<p>A Survey on Bias and Fairness in Machine Learning</p>
						<p><small><a href="https://arxiv.org/pdf/1908.09635.pdf" target="_blank">https://arxiv.org/pdf/1908.09635.pdf</a></small></p>
					</section>	
				</section>				
				<section>
					<section>
						<h1>Section 3</h1>
						<h3 class="fragment">Final observations</h3>
					</section>
					<section data-background-image="img/taxonomy.jpg">
					</section>
					<section>
						<h2>Bias is not one thing</h2>
						<h2 class="fragment">Mitigation techniques require additional disciplines - social sciences &amp; ethics</h2>
						<aside class="notes">Remember the map.</aside>
					</section>
					<section>
						<h2>Bias is not one thing</h2>
						<ol>
							<li class="fragment">First identify</li>
							<li class="fragment">Then measure</li>
							<li class="fragment">Finally mitigate</li>
						</ol>
						<aside class="notes">Remember the map.</aside>
					</section>
					<section>
						<h2>Acknowledging failure</h2>
						<h3>(is inevitable)</h3>
						<aside class="notes">Common behaviour is to think of ourselves as ethical and therefor reject the idea that our systems can have a harmful impact on the world</aside>
					</section>
					<section>
						<blockquote class="strikethrough">"I am an ethical person therefore I build ethical tech"</blockquote>
					</section>
					<section>
						If you write a bug, it doesn't make you a bad programmer 
						<aside class="notes"></aside>
					</section>
					<section>
						If you make an ethical mistake, it doesn't make you an unethical person
						<aside class="notes"></aside>
					</section>
					<section>
						<h3>Let's draw a line in the sand</h3>
						<aside class="notes">We can be flawed, squishy and still aspire to ethical tech just as we are emotional meat robots who still aspire to write great code.</aside>
					</section>
					<section data-background-image="img/line-in-sand.jpeg">
						<h2 class="strikethrough">Claiming moral authority</h2>
						<br>&nbsp;<br>&nbsp;<br>
						<h2>Claiming moral imperative</h2>
						<aside class="notes">Difference between saying we know, we are right, and saying we must try, we must do the work </aside>
					</section>
					<section data-background-image="img/Tactile_9.jpg">
						<h2>Acknowledging complexity</h2>
						<aside class="notes">I’m not going to pretend that this is easy or that the work is clear cut and unambiguous. It’s not. But all the same it’s real engineering work. It’s about identifying trade offs, working through use cases, and making difficult compromises. It’s about staring unflinchingly at complex systems and refusing to blink. </aside>
					</section>
					<section>
						<h2>Fairness activities, definitions, worksheets</h2>
						<p><a href="https://tinyletter.com/summerscope" target="_blank">tinyletter.com/summerscope</a></p>
					</section>
					<section data-background-color="#ffffff" data-background-image="img/Tactile_7.jpg">
						<h1>Thank you!</h1>
						<p><small><a href="http://summerscope.github.io/slides/elusiveness-of-ethics-rts" target="_blank">summerscope.github.io/slides/elusiveness-of-ethics-rts</a></small></p>
					</section>
					<section>
						<h1>Questions?</h1>
					</section>
				</section>		
				<!-- <aside class="notes"></aside> -->
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				backgroundTransition: 'none', // none/fade/slide/convex/concave/zoom
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
