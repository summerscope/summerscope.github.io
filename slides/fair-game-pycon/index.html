<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<meta name="description" content="How do we connect high-level principles with day-to-day product decision making? How do we move past the AI Ethics hype and start trying, testing and implementing practical approaches? These questions are at the heart of Laura's work, and in this talk she shares stories, discoveries, and decisions from her time as an 'ethics ops' consultant embedded with a small team in a big telco. From improving the science bit of data science, to developing the collective sensitivity of the team, to designing recourse for false positives, tune in for pragmatic pointers and actionable take-aways that you can try with your team right away.">
		<title>Fair game: the ethics of telco fraud</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css" id="theme">
		<link href="https://fonts.googleapis.com/css2?family=Krub:wght@200;300;400;600&family=Lora:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<img src="img/fair-game.png" alt="Fair game" />
					<h2>The ethics of telco fraud</h2>
				</section>
				<section>
					<p>Slides &raquo; <br/><small><a href="http://summerscope.github.io/slides/fair-game-pycon/" target="_blank">summerscope.github.io/slides/fair-game-pycon</a></small>
					<aside class="notes">Find link to my slides and all the links / references I'm mentioning in google doc. Also drop your questions in here! </aside>
				</section>
				<section>
					<h2>Lineup</h2>
 					<ol class="smallText">
						<li class="fragment">Deconstruct your proxy</li>
						<li class="fragment">Explicit data design</li>
						<li class="fragment">Assume good faith</li>
						<li class="fragment">Failure-first design</li>
					</ol>
					<aside class="notes">Four primary findings I'll walk you through. These apply to any kind of automation or decision system. Not telco or fraud-specific, they apply to you!</aside>
				</section>
				<section>
					<h2>Setting the scene</h2>
					<aside class="notes">Starting early 2020, I embedded as an EthicsOps consultant in a small fraud detection team in a telco. ~1yr. 1 DS, 1 Ops, 1 PM, 1 Data Engineer, plus. Using a combo of internal tools and 3rd party payment gateway (ML) for fraud detection. Can't share specifics on features or thresholds. Looking primarily for international revenue sharing but also to a lesser extent service overuse and promotions abuse. </aside>
				</section>				
				<!--<section>
					<h2>Ethics ops consulting</h2>
					<p><a href="https://debias.ai" target="blank">debias.ai</a></p>
					<aside class="notes">
						At the beginning of 2020 (which is either a year or a lifetime ago, who even knows) I began consulting with a telco for Debias AI. I am primarily embedded in their fraud detection team. Their approach to fraud detection is pretty standard - a mix of monitoring, internal business rules, and third party systems with proprietary fraud ML algorithms.
					</aside>
				</section>-->
				<section>
					<img src="img/harm-lenses.png" width="700" alt="Three eye glasses with labels, User Harm, Historical Bias, Algorithmic Bias" />
					<aside class="notes">
						EthicsOps - think like DevOps or SecOps - tooling and process to support ethics. Or like SREs - building for resiliancy but with an awareness never 100% up time - it's never 'done' or 100% perfect.
					</aside>
				</section>
				<section>
					<h2>A balancing act</h2>
					<aside class="notes">
						Fraud is an interesting space in which to discuss ethics. Not only are you (the company) capable of inflicting harm on your customers, you are also under attack by a sub-set of customers who mean you harm. You have to balance the real, often urgent concerns of identifying bad behaviour & halting costs while also acknowledging the possibility of false positives, and considering how you might identify and support any innocent parties who accidentally get caught in the cross-fire. It's a war of attrition - like email spam. 
					</aside>
				</section>
				<section>
					<h2>Developing a healthy cynicism about data sources</h2>
					<aside class="notes">When we start to feel really comfortable is usually when we discover something can be totally different than our expected interpretation.</aside>
				</section>
				<section>
					<h2>Data (interpretation) pride comes before a fall</h2>
					<aside class="notes">
						Anecdote #1: The textivist... (NB Marketing use - overseas texting is EXPENSIVE and outside of most terms of use for personal plans). 
					</aside>
				</section>
				<section>
					<h1 class="learning">Finding #1</h1>
					<h2>Deconstruct your proxy</h2>
					<aside class="notes">All data is a proxy - it's an approximation of the thing we actually want to measure. (time allowing - Apple CC example). </aside>
				</section>
				<section>
					<h2>A concept framework</h2>
				</section>
				<section>
					<div class="columns">
						<div class="fragment">
							<img class="clean" src="img/signal.png" height="400" alt="" />
							<h3>Signal</h3>
						</div>
						<div class="fragment">
							<img class="clean" src="img/arrow.png" height="400" alt="arrow" />
							<br>
							<br>
							infers
						</div>
						<div class="fragment">
							<img class="clean" src="img/activity.png" height="400" alt="" />
							<h3>Activity</h3>
						</div>
						<div class="fragment">
							<img class="clean" src="img/arrow2.png" height="400" alt="arrows" />
							<br>
							<br>
							infers
						</div>
						<div class="fragment">
							<img class="clean" src="img/persona.png"  height="400"  alt="" />
							<h3>Persona</h3>
						</div>
					</div>
					<aside class="notes">
						Like a reverse persona. Working backwards from the data. It helps us be explicit about the logic leaps we are making. Always asking the question how good a proxy is this data for the thing I REALLY want to know? Don't forget we also measure anti-signals.
					</aside>
				</section>
				<!-- <section>
					<h1 class="learning">Finding #2</h1> 
					<h2>Develop a healthy cynicism about data sources</h2>
					<aside class="notes">
						This is really an extension of the first learning / point. 
					</aside>
				</section> -->
				<!-- <section>
					<h2>Stanislav Petrov</h2>
					<img src="img/stanislav-petrov.jpg" width="600" />
					<p>"The man who saved the world"</p>
					<aside class="notes">
						On Sept. 26, 1983, Oko ( the Soviet Union’s early-warning satellite system for nuclear attack) detected that the United States had launched five ballistic missiles, all headed toward the USSR. They had a fixed rule (business logic) to report this which would have resulted in missile launch - but he didn't.
						Stanislav was an engineer before he was an army man, and I think this is a story about the deep intuition of a technician. Someone who can put their hand on the bonnet and know how well the machine is running.
					</aside>
				</section>
				<section>
					<blockquote>"The false alarm had picked up the sun's reflection atop the clouds, mistaking it for a missile launch."</blockquote>
					<br>
					<p><small>— <a href="https://www.ajc.com/news/world/the-man-who-saved-the-world-died-and-the-world-didn-notice-who-was-stanislav-petrov/0EgMxqN7DNmCNiI89g4gSI/" target="_blank">'The man who saved the world' died and the world didn't notice — Who was Stanislav Petrov?</a></small></p>
				</section> 
				<section>
					<h2>A $600m mistake?</h2>
					<blockquote>""[The dataset] is an index of Australian addresses. The dataset does not contain a list of premises," he said."</blockquote>
					<br>
					<p><small>— <a href="https://www.abc.net.au/news/2020-11-10/nbn-co-spends-$600m-on-finishing-rollout/12844236" target="_blank">Another 300k NBN connections to cost $600m, CEO blames blowout on bad address data</a></small></p>
					<aside class="notes">NBN budget increased to deal with 300k premises that were not captured in their original sweep. Imagine being the unit on a block which was overlooked? Or being the last on your street to get upgraded? This impacts work, study, house prices, lives.</aside>
				</section>
				<section>
					<h2>Look at your raw data</h2>
					<h3 class="fragment">Not just at dashboards</h3>
					<aside class="notes">Data scientists please use your tools to interrogate raw data validity: does it actually meet our expectations? Types, formatting, data quality? Don't just examine 1st 10 results in a data table. SMEs & everyone else - get up close and personal with incoming data too, look for the human stories behind the data. You'll be surprised what you learn! </aside>
				</section>-->
				<section>
					<h1 class="learning">Finding #2</h1>
					<h2>Explicit data design</h2>
					<aside class="notes">Let's make a distinction between exploratory data science, and load bearing outputs.</aside>
				</section>
				<section>
					<blockquote>Data misinterpretation is <u>easy</u>, and <u>likely</u> under pressure</blockquote>				
					<aside class="notes">One of the most interesting things about working with fraud is the pressure it puts on data interpretation in periods of attack. Like putting data science hygiene through the wringer -  in unexpected bursts. </aside>
				</section>
				<!--<section>
					<h2>For example...</h2>
					<aside class="notes">
						Maintenance outages (planned) causing delays in data stream. Fixed delay in timeframe from data ingest > running feature > alert in slack - got us every time! 
					</aside>
				</section>-->
				<section>
					<h2>Design for cognitive load</h2>
					<h3 class="fragment">Simple doesn't imply you're&nbsp;stupid</h3>
					<aside class="notes">
						Anecdote #2 - the batch delay. Fixed delay in timeframe from data ingest > running feature > alert in slack - got us every time! | Wherever possible, add labels. Be explicit. Make it impossible to misinterpret. Just because your team is smart, technical, doesn't mean you want them to have to be brilliant detectives under time and cost pressure.
					</aside>
				</section>
				<!--<section>
					<h2>Litmus test</h2>
					<h3>On a quick glance, is this easy to misunderstand?</h3>
					<aside class="notes">
						Even if you're not working in the space of fraud, we experience this same kind of pressure all the time. When a customer discovers a bug in production. When there's an outage with a third party, etc. 
					</aside>
				</section>-->
				<section>
					<h2>Pointers for data interpretation</h2>				
					<aside class="notes"></aside>	
				</section>
				<section>
					<h2>Avoid DSL <br/>(domain specific language)</h2>
					<img class="fragment" src="img/data-dsl.png" width="700" alt="" />
				</section>
				<section>
					<h2>Add units</h2>
					<img class="fragment" src="img/data-units.png" width="700" alt="" />
				</section>
				<section>
					<h2>Support percentiles with absolute (total count)</h2>
					<img class="fragment" src="img/data-absolute.png" width="700" alt="" />
				</section>
				<section>
					<h2>Make the data recency clear</h2>
					<img class="fragment" src="img/data-recency.png" width="700" alt="" />
				</section>
				<!-- <section>
					<h1 class="learning">Finding #3</h1> 
					<h2>Holding space for uncertainty <br>(Without being paralysed)</h2>
					<aside class="notes">We want to acknowledge the uncertainty inherent in our decision making without allowing us to be paralysed</aside>
				</section> -->
				<!-- <section>
					<h2>Let's talk about probabilities</h2>
					<aside class="notes">Talking about probabilities in a cross-functional team is... hard. Everyone will have a different level of stats and data literacy. Acknowledge this. Start with simple ways of observing and discussing confidence, don't jump into the deep end. </aside>
				</section>
				<section>
					<img src="img/confidence.png" width="600" alt="Different approaches to measuring confidence and probabilities" />  
					<aside class="notes">Lots of different levels of fidelity. What's important is that we expose the uncertainty, and the scale of that uncertainy. Always circle back to absolutes to ground the relative scores. 90% accurate is great in a data set of 10 points. 99% accurate means 50,000 wrong answers in a data set of 5 million.</aside>
				</section> -->
				<!-- <section>
					<h2>Deterministic vs Stochastic</h2>
					<h3 class="fragment">There will be noise either way</h3>
					<aside class="notes">A lot is made about the difference between deterministic programming (where the code is fixed) and stochastic. But in any complex system there will be noise, and unexpected effects. Any automation should expect to grapple with unintended outcomes, false positives. </aside>
				</section>
				<section>
					<h2>Machine Learning is statistics</h2>
					<aside class="notes">And Stats is based on the stochastic (noisy) universe. If you are bulding ML, the ONLY certainty is that some of the engine's outputs will be wrong. If you can't afford to design for decision review and decision recourse then you can't afford to design an ML model to use in prod.</aside>
				</section>
				<section>
					<h2>Noise</h2>
					<p>A new book coming soon by Daniel Kahneman</p>
					<p><small><a href="https://www.bookdepository.com/Noise-Daniel-Kahneman/9780008308995" target="_blank">See on book depository</a></small></p>
					<aside class="notes">Importance of distinguising noise from bias</aside>
				</section> -->
				<!--<section>
					<h1 class="learning">Finding #2</h1>
					<h2>Mapping knowledge states</h2>
					<aside class="notes">ok. so you probably knew this was coming. This is the section where i tell you that the best way to improve your data science practice as a whole is to do better, much better, at the boring science bits.</aside>
				</section>
				<section data-background-image="img/assumptions-swamp.jpg">
					<h2>Beware the swamp of lazy assumptions</h2>
					<aside class="notes">Anecdote: when joined telco there was a lot of confusion about what types of fraud were, and what they were called. Names were quite confusing, unintuitive, imprecise language, name overlaps! I wasn't the only one who was confused.</aside>
				</section>
				<section>
					<h2>Implicit vs Explicit knowledge</h2>
					<h3 class="fragment">Become a knowledge excavator</h3>
					<aside class="notes">Just because a thing is knowable, doesn't mean it is known. Often the first step to establishign a shared understanding is to sniff out this existing implicit knowledge and formalise it.  </aside>
				</section>
				<section>
					<h2>Naming matters</h2>
					<ul>
						<li class="fragment">Build a shared vocabulary</li>
						<li class="fragment">Build shared mental models</li>
						<li class="fragment">Avoid name-space clashing</li>
					</ul>
				</section>
				<section>
					<h2>For example</h2>
					<blockquote>International revenue sharing fraud <br>-vs-<br> Toll fraud</blockquote>
					<aside class="notes">these are confusingly different names for the same thing unrelated to Call Reselling (but these often get confused)</aside>
				</section>
				<section>
					<h2>Personas</h2>
					<img src="img/mystery-marketer.png" width="600" alt="Person shouting through a bullhorn with lots of chat bubbles coming out" />
					<aside class="notes">For example - a persona for people who overuse promotions which describes a 'deal seeking behaviour', very different to the persona of someone trying to pose as a telco and on-sell services. We established a shared model and vocab that stakeholders from support and ops to data science can use.</aside>
				</section>
				<section>
					<h2>Defining your baseline</h2>
					<aside class="notes">If you don't have a shared understanding it's hard to move forwards. Needed for all experiments, hypotheses you want to try moving forwards. Also - perhaps minor discrepancies in what is the current state will reveal a question to investigate? </aside>
				</section>
				<section>
					<h2>For example</h2>
					<h3>Looking at the logs</h3>
					<aside class="notes">Looking at the history of what actions had been taken on a service number - can make longitudinal analysis very difficult if the reasons aren't well documented AND if there is no way to observe the occurance of false positives. If you don't know when you automation was wrong, hard to use that data with confidence. Also - maybe your logs aren't the source of truth, ok! as long as somewhere there is a source of truth. </aside>
				</section> 
				<section>
					<h2>Data schemas</h2>
					<img src="img/data-schema.png" width="700" alt="Grid with three columns, What did I think, New info, What changed" />
					<aside class="notes">For internal experiments. Take the job of data capture about your customers seriously. Take the time to think about what fields, what meta data will be useful for investigations. Future you will thank you.</aside>
				</section>
				<section>
					<h2>Changing our language <br>changes our minds</h2>
					<blockquote class="fragment">"We'll wait until we're 100% certain... no make that 99% certain"</blockquote>
					<aside class="notes">Colleague who has started to change his language </aside>
				</section>-->
				<section>
					<h1 class="learning">Finding #3</h1>
					<h2>Assume good faith</h2>
					<aside class="notes">A common problem whenever you work with bad actors is to become suspicious, and to start seeing malfeasance everywhere. No need to suspect bad intentions when laziness or incompetance will do.</aside>
				</section>
				<section>
					<h2>Added uncertainty in the space of fraud</h2>
					<h3 class="fragment">You <em>really</em> can't trust what people say</h3>
					<aside class="notes">In UXR you develop a sniff test for honesty and vulnerability vs bulshit answers. But in the fraud space you had this added complexity of knowing that sometimes customers may be lying or trying to game the system to meet their own ends. Dealing with this day-in day-out can make these discussions feel instantly suspect, and you'll start to see fraud everywhere. </aside>
				</section>
				<section>
					<h2>The antidote to suspicion</h2>
					<h3 class="fragment">Setting the intention to be respectful</h3>
					<aside class="notes">The best way to avoid falling into this trap is to be explicitly, deliberately kind and respectful in all our communications</aside>
				</section>
				<section>
					<h2>bUt wHAt iF iT's A BAd AcTor?</h2>
					<h3 class="fragment">So what?</h3>
					<aside class="notes">What is the cost of all outcomes? Being respectful to someone who is trying to rip you off vs speaking harshly to a legit customer by accident? Or disconnecting a bad actor (save $) vs a legit or differently abled customer (lose $). Also consider also reputational harms. Focus on your automation goal.</aside>
				</section>
				<section>
					<h2>Keep the moralising <u>out of it</u></h2>
					<aside class="notes">Being 'right' isn't helpful. You don't know that person, you don't know their story. </aside>
				</section>
				<section>
					<h2>Tip</h2>
					<h3>Describe <em>behaviours</em>, not <em>people</em></h3>
					<aside class="notes">Describe behaviour (service misuse) not people (fraudster). There's literally no value in applying a moralising title to a person. THere's every possibility of harm, on both sides, if you get it wrong.</aside>
				</section>
				<section>
					<h2>👉 You can still set boundaries</h2>
					<aside class="notes">Don't mistake my meaning for 'you should let people do whatever they want'. Being kind and calm does not imply letting people get away with bad behaviour or failing to set boundaries in your design. </aside>
				</section>
				<section>
					<h1 class="learning">Finding #4</h1>
					<h2>Failure-first design</h2>
					<aside class="notes"></aside>
				</section>
				<!--<section>
					<h2>If you don't ask,<br /> customers won't tell</h2>
					<img src="img/too-hard-basket.png" width="200" alt="Woven basket labeled Too Hard" />
					<aside class="notes">If your system makes an incorrect assumption, are you in a position to find out? I think of this as the rule of lurkers vs contributors. Most people won't bother to give you feedback.  In the space of fraud, if you have taken an action on a service they're more likely to churn than to make an effort to reach out to you - and why would they bother based on a market where churn is the norm and it's easy to swap providers? If they are really angry you might get feedback - but through the form of a complaint to the TIO (the regulator) - also a bad outcome. </aside>
				</section>
				<section>
					<h2>Customers <u>are</u> the experts...</h2>
					<h3 class="fragment">...on their own experience</h3>
					<aside class="notes"></aside>
				</section>
				<section>
					<h2>Feedback loops must be </h2>
					<ul>
						<li class="fragment">Intuitive</li>
						<li class="fragment">Contextual</li>
						<li class="fragment">Timely</li>
					</ul>
					<aside class="notes">I'm a fan of micro-feedback, small and contextual questions that are really lightweight and easy to answer.</aside>
				</section>
				<section>
					<h2>Plan time for...</h2>
					<ul>
						<li class="fragment">Customer support</li>
						<li class="fragment">Product/model improvements</li>
						<li class="fragment">Integrating your learnings</li>
					</ul>
				</section>
				<section>
					<h2>Be explicit</h2>
					<h3 class="fragment">If you can't imagine consequences, you're not thinking hard enough</h3>
					<aside class="notes">The best way to prepare yourself to design in this world is to be ultra explicit about the goals of your system, and the possible harms that might occur to your end users. </aside>
				</section>
				<section>
					<h2>Shot</h2>
					<blockquote>We believe this system has contributed to making Facebook the safest place on the Internet for people and their information.</blockquote>
					<br>
					<p><small>— <a href="https://research.fb.com/publications/facebook-immune-system/" target="_blank">Facebook Immune System by Tao Stein, Roger Chen, Karan Mangla</a></small></p>
					<aside class="notes">FB Research paper from 2011. :Screams_forever: </aside>
				</section>
				<section>
					<h2>Chaser</h2>
					<blockquote>"The goal is to protect the graph against all attacks rather than to maximize the accuracy of any one specific classifier. The opportunity cost of refining a model for one attack may be increasing the detection and response on other attacks."</blockquote>
					<br>
					<p><small>— <a href="https://research.fb.com/publications/facebook-immune-system/" target="_blank">Facebook Immune System by Tao Stein, Roger Chen, Karan Mangla</a></small></p>
					<aside class="notes">:Screams_forever: </aside>
				</section>-->
				<!-- <section>
					<h2>Your system <u>will</u> get it wrong</h2>
					<h3 class="fragment">So how will it handle failure?</h3>
					<aside class="notes">Start from the assumption that your system is not perfect. We know this is the case with any machine learning or statistical models. The literal only assumption you can make is that it will be wrong for some percent of the time. And ever removing the issue of statistical models there are plenty of other aspects of your tech stack and human stack that point to the strong certainty of some amount of failure: complexity, culture, etc.</aside>
				</section> -->
				<!-- <section>
					<h2>For example</h2>
					<aside class="notes">In the land of fraud the goal is to prevent the costs of service misuse and fraud. The harms are when we take action that can result in a user unexpectedly losing service. </aside>
				</section> 
				<section>
					<h2>When a system fails...</h2>
					<h3 class="fragment">...There are always consequences</h2>
					<aside class="notes">Remember the FB paper? Circle back here if you need convincing.</aside>
				</section>-->	
				<section>
					<blockquote>"Assuming perfect knowledge of the objective decouples the machine from the human: what the human does no longer matters, because the machine knows the goal and pursues it."</blockquote>
					<p><small>-Stuart Russell, Human Compatible</small></p>
					<aside class="notes">No perfect, bug-free software. Add in data, stochastic universe - more uncertainty. Designing objectives or incentives for ML systems - further complexity. So let's start from the pragmatic view: that our system will fail, and we need to find out when they do.</aside>
				</section>
				<section>
					<img src="img/escape-hatch.png" width="300" alt="Escape hatch" />
					<h2>Designing an escape hatch</h2>
					<aside class="notes">Like designing an escape hatch, or adding life boats. You don't want to have to use it, but if you need it you sure hope it's there. Anecdote #3 - finding out about a service mistaken disconnected... from the ombudsman.</aside>
				</section>
				<section>
					<h2>Feedback loops must be</h2>
					<ul>
						<li class="fragment">Intuitive</li>
						<li class="fragment">Contextual</li>
						<li class="fragment">Timely</li>
					</ul>
					<aside class="notes">I'm a fan of micro-feedback, small and contextual questions that are really lightweight and easy to answer.</aside>
				</section>
				<section>
					<h2>Integrate into UI</h2>
					<img class="fragment" src="img/design-help.png" width="700" alt="" />
				</section>
				<section>
					<h2>Confirm your classifications</h2>
					<img class="fragment" src="img/design-photos.png" width="400" alt="" />
				</section>
				<section>
					<h2>Collaborative security</h2>
					<img class="fragment" src="img/design-email.png" width="400" alt="" />
					<aside class="notes">Calling back to previous section - don't assume fraud if phishing will do</aside>
				</section>
				<section>
					<h2>Just ask</h2>
					<img class="fragment" src="img/design-ask.png" width="400" alt="" />
				</section>
				<section>
					<h2>Think the interaction through</h2>
					<ul>
						<li class="fragment">Plan for customer support</li>
						<li class="fragment">Capture your learnings</li>
						<li class="fragment">Product/model improvements</li>
					</ul>
				</section>				
				<!--<section>
					<h2>Litmus test</h2>
					<h3>What if this happened to my most vulnerable customer?</h3>
					<aside class="notes">In the space of a telco, we're asking what could go wrong with a mobile service level action. I find this question useful for thinking about service level actions. It helps frame the cost of false positives and makes us more focused on the minimum viable intervention needed to achieve our goals of capping these costs.</aside>
				</section>
				<section>
					<h2>Litmus test</h2>
					<h3>Could any customer go through my escape hatch, recover and remain a happy customer?</h3>
					<aside class="notes">Sometimes you have to think creatively - pitch a friction as a value add. "We are working hard to protect your identity and your account's safety" is a good one, for instance. Test out your explanations and flows on random people NOT those who match the assumptions your system made as a way to assess the quality of the solution.</aside>
				</section>
				<section>-->
				<section>
					<h2>Prepare for pushback</h2>
					<ul>
						<li class="fragment">People don't like it when you make bad assumptions about them!</li>
						<li class="fragment">Making the implicit explict is going to be uncomfortable</li>
					</ul>
					<aside class="notes">This kind of work can throw up lots of challenges - differences in mental models between what people think they're buying and what you think you're selling. We did some experiments sending comms to provide clearer feedback to people whose service came under suspicion, and allow for them to request support if they think there has been a mistake. We got a LOT of pushback. Some very angry customers.</aside>
				</section>
				<section>
					<h2>Harm mapping</h2>
					<h3 class="fragment">Build your capacity for design speculation &amp; imagining possible harms</h3>
					<p class="fragment"><a href="https://github.com/summerscope/mapping-fair-ml" target="_blank">github.com/summerscope/mapping-fair-ml</a></p>
					<aside class="notes">This is a whole topic but just to point to it, this is the bit where we think about what we might get wrong, and what are the consequences. There are a whole bunch of tools and frameworks out there to try - ethics litmus tests among them.</aside>
				</section>		
				<!--<section>	
					<blockquote>"Delays in feedback loops are common causes of oscillations. If you're trying to adjust a system state to your goal, but you only receive delayed information about what the system state is, you will overshoot and undershoot."</blockquote>
					<br>
					<p><small>- <a href="http://donellameadows.org/wp-content/userfiles/Leverage_Points.pdf" target="_blank">Leverage Points: Places to Intervene in a System by Donella Meadows</a></small></p>
					<aside class="notes">The escape hatch is also your feedback loop telling you about the state of your system.</aside>
				</section>
				<section>
					<h2>Rethinking automation design</h2>
					<aside class="notes">So many options we don't need to think about automations as all or nothing</aside>
				</section>
				<section>
					<h2>Breaking down the monolith</h2>
					<ul>
						<li class="fragment">Concierge prototypes (human-as-bot)</li>
						<li class="fragment">Automations can be temporary or time-boxed</li>
						<li class="fragment">Dashboard? Notification? Escalation? Action?</li>
						<li class="fragment">Do usability testing (duh)</li>
					</ul>
					<aside class="notes">If it feels weird/wrong as a human interaction, it probably means it's bad as an automation.</aside>
				</section>-->
				<!-- <section>
					<h1 class="learning">Finding #7</h1>
					<h2>Speed matters</h2>
					<aside class="notes">Speed of feedback loops naturally flows into another classic product question - how fast should we build and deploy things</aside>
				</section>
				<section>						
					<img src="img/fast-slow.png" alt="Spectrum showing two extremes in product - too fast and too slow" />
					<br><br>
					<p class="fragment">The challenge is finding the sweet spot</p>
					<aside class="notes">Too slow can be just as dangerous as too fast. Design by committee is a painful experience. </aside>
				</section>
				<section>
					<h2>Dangers of too slow</h2>
					<ul>
						<li class="fragment">Feedback takes too long</li>
						<li class="fragment">You may not see the impact of a decision</li>
						<li class="fragment">Doing things <em>feels hard</em></li>
						<li class="fragment">Culture of resistance</li>
						<li class="fragment">Diffusion of responsibility</li>
						<li class="fragment">Get anchored to your first idea</li>
						<li class="fragment">Sunk cost bias</li>
					</ul>
					<aside class="notes">Much more difficult to treat solutions / features / ideas lightly, as hypotheses to be validated or not. </aside>
				</section>
				<section>
					<h2>Dangers of too fast</h2>
					<ul>
						<li class="fragment">Ask forgiveness not permission</li>
						<li class="fragment">Insufficient knowledge / expertise</li>
						<li class="fragment">Overconfidence</li>
						<li class="fragment">Failing to measure impact</li>
						<li class="fragment">Reinventing the wheel</li>
					</ul>
					<aside class="notes">"The Facebook" (especially early years)</aside>
				</section> -->							
				<section>
					<h1>Final thoughts</h1>
				</section>
				<section>
					<h2 class="learning">Findings</h2>
					<ol class="smallText">
						<li>Deconstruct your proxy</li>
						<li>Explicit data design</li>
						<li>Assume good faith</li>
						<li>Failure-first design</li>
					</ol>					
					<aside class="notes">Let's take a moment to remember these findings we talked through.</aside>
				</section>			
				<section>
					<img src="img/bike-shed.png" width="400" alt="" />
					<blockquote>Applied approaches > generalised&nbsp;principles</blockquote>
					<aside class="notes">
						 30 second sommary of other talk on phenomenon of AI Ethics Principles / Corporate Values. The work is in the detail, the context, the daily decisions. The desire to abstract or to generalise, can itself be a form of procrastination.  
					</aside>
				</section>
				<section data-background-image="img/hair.gif">
					<br><br><br><br><br><br><br><br><br><br><br>
					<p><small>— Gif by <a href="https://giphy.com/Barbara_Pozzi" target="_blank">Barbara Pozzi</a></small></p>
					<aside class="notes">
						A metaphor: sailing in a big cruise liner. When you're on the passenger deck it's quiet, safe, boring even. No sense of motion. When you go out on deck you see the speed, the mass of water displaced, etc... Sometimes we *want* to feel the wind rushing past. I think insulating ourselves from that sensation is unhelpful. 
					</aside>
				</section>	
				<!--<section>
					<blockquote>"The first principle is that you must not fool yourself – and you are the easiest person to fool."</blockquote>
					<p>— Richard Feynman</p>
				</section>-->
				<section>
					<h1>The real game</h1>
					<h3 class="fragment">Trying to catch fraud, <br> yet not defraud ourselves </h3>					
					<aside class="notes">Our minds, our assumptions too, are fair game. We have more in common with the fraudsters than we might think.
						We are driven by the same incentives as the fraudsters - capitalism played out in light and dark markets. </aside>
				</section>
				<section>
					<h2>Thank you</h2>
					<p>Slides &raquo; <br/><small><a href="http://summerscope.github.io/slides/fair-game-pycon/" target="_blank">summerscope.github.io/slides/fair-game-pycon</a></small>
				</section>				
			</div>

			<!-- Twitter perptual anchor-->
			<div class="twitter">
				<a href="https://twitter.com/summerscope">@summerscope</a>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
