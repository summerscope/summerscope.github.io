<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Is AI ethics just... ethics?</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1>Is AI ethics just... ethics?</h1> 
				</section>
				<section>
					<h2>About me</h2>
					<img class="plain" src="img/foundress.png" height="400px" />
					<aside class="notes">About me - design background, learning deep learning & ML, reading papers, Fair ML reading group</aside>
				</section>
				<section data-background-image="img/CardMocks9.jpg">
				</section>
				<section>
					<h2>Ethics Litmus Tests</h2>
					<p><a href="https://ethical-litmus.site" target="_blank">ethical-litmus.site</a></p>
					<aside class="notes">Quick shameless plug</aside>
				</section>
				<section>
					<h2>Slides</h2>
					<p><small><a href="http://summerscope.github.io/slides/is-ai-ethics-just-ethics" target="_blank">summerscope.github.io/slides/is-ai-ethics-just-ethics</a></small></p>
					<aside class="notes">Links all live, don't stress about trying to write down URLs</aside>
				</section>
				<section>
					<h2>What is AI? What is Machine Learning?</h2>
					<aside class="notes">Get a sense of the knowledge in the room. Definitions if needed</aside>
				</section>
				<section>
					<h1>So...</h1>
					<h2 class="fragment">Is AI ethics just... ethics?</h2>
					<aside class="notes">Discuss - what do you think? What is ethics? What is distinct about AI ethics?</aside>
				</section>
				<section>
					<p>put another way...</p>
					<h2 class="fragment">Has ML introduced novel ethics problems?</h2>
				</section>
				<section>
					<blockquote>
						"technology's role as a social, political and environmental accelerant"
					</blockquote>
					<p>- <a href="https://newdesigncongress.org/" target="_blank">newdesigncongress.org</a></p>
				</section>
				<section>
					<h3>Same old challenges of software engineering...</h3>
					<h3 class="fragment">...now with added consequences<sup>TM</sup></h3>
				</section>
				<!-- Scale -->
				<section>
					<h1>Problems of scale</h1>
				</section>
				<section>
					<h2>For example... </h2>
					<img class="plain" src="img/google-photos.png" alt="Google Photos" />
					<aside class="notes">Example of amount of human time needed to assess and make a judgement about ML classifications from a single model.</aside>
				</section>
				<section>
					<ul>
						<li><strong>1.2 billion</strong> photos per day uploaded to Google photos in 2017</li>
						<li class="fragment">Manually labelling each one would take well over 1 million people working full time</li>
						<li class="fragment">If <strong>1%</strong> of those label decisions were flagged for human review and each review took <strong>3 minutes</strong>, it would require <strong>150,000</strong> people working full time (Google currently employs 103,459 people)</li>
					</ul>
				</section>
				<!-- Abstraction layers -->
				<section>
					<h1>New abstraction layers</h1>
				</section>
				<section>
					<img class="plain" src="img/17Layers.gif" height="550" />
					<p><small><a href="http://geekswithblogs.net/TimothyK/archive/2019/07/30/the-17-layered-app.aspx" target="_blank">geekswithblogs.net/TimothyK/archive/2019/07/30/the-17-layered-app</a></small></p>
				</section>
				<section>
					<img class="plain" src="img/17Layers+extra.png" height="550" />
				</section>
				<section>
					<img class="plain" src="img/black-boxes.png" height="550" />
					<aside class="notes">Who's heard of black boxes? Any questions about what that means in the context of ML?</aside>
				</section>
				<!-- Bias and inequality -->
				<section>
					<h1>Problems of bias and inequality</h1>
				</section>
				<section>
					<h3>Structural inequality may not be a new problem...</h3>
					<h3 class="fragment">...but many of these techniques are</h3>
				</section>
				<section>
					<img src="img/models.jpg" height="550" />
				</section>
				<!-- Responsibility -->
				<section>
					<h1>Problems of Responsibility</h1>
				</section>
				<section>
					<h3>"The shakey spear"</h3>
				</section>
				<section>
					<h3>The locus of moral agency</h3>
				</section>
				<section>
					<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I truly do not understand why so many AI luminaries see the drive for interpretability as a threat.<br><br>If this AI surgeon killed your mom, you’d probably want to know if it was preventable. A defect. Someone’s fault. That’s how our legal system works. If AI can’t work w/our laws... <a href="https://t.co/QgxMNjF2H4">https://t.co/QgxMNjF2H4</a></p>&mdash; Liz O’Sullivan (@lizjosullivan) <a href="https://twitter.com/lizjosullivan/status/1230897634958880768?ref_src=twsrc%5Etfw">February 21, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				</section>
				<section>
					<h3>Moral crumple zone</h3>
					<p class="fragment"><a href="https://www.semanticscholar.org/paper/Moral-Crumple-Zones%3A-Cautionary-Tales-in-Elish/4fc0d366d9ffd491d5955e7252edc4bf5f274291" target="_blank">Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction</a></p>
				</section>
				<section>
					<img class="plain" src="img/spectrum.png" /> 
				</section>
				<!-- Harm mitigation -->
				<section>
					<h1>Harm mitigation is... complicated</h1>
				</section>
				<section>
					<h3>Interpretability</h3>
					<p class="fragment">-vs-</p>
					<h3 class="fragment">Explanation</h3>
					<p class="fragment">-vs-</p>
					<h3 class="fragment">Recourse</h3>
				</section>
				<section>
					<h3>Interpretability</h3>
					<p><small>Also called Explainability</small></p>
					<blockquote>Do we know how the algorithm arrived at a specific decision?</blockquote>
				</section>
				<section>
					<p>LIME paper - husky vs wolf</p>
					<img src="img/husky-vs-wolf.png" height="400" />
					<p><small><a href="https://arxiv.org/pdf/1602.04938.pdf" target="_blank">arxiv.org/pdf/1602.04938.pdf</a></small></p>
				</section>
				<section>
					<h3>Explanation</h3>
					<blockquote>Can we meaningfully explain this decision process to the people using the system?</blockquote>
					<aside class="notes">Who has written help copy or UX micro copy? error copy? Who's done usability testing? well...</aside>
				</section>
				<section>
					<blockquote>
						Much of this research is focused on explicitly explaining decisions or actions to a human observer... However, it is fair to say that most work in explainable artificial intelligence uses <strong>only the researchers’ intuition</strong> of what constitutes a ‘good’ explanation.
					</blockquote>
					<p><small>- Tim Miller <a href="https://arxiv.org/pdf/1706.07269.pdf" target="_blank">arxiv.org/pdf/1706.07269.pdf</a></small></p>
				</section>
				<section>
					<h3>Recourse</h3>
					<blockquote>Can we provide corrective steps that users can take to address an incorrect classification? </blockquote>
				</section>
				<section>
					<img src="img/recourse.png" height="550">
					<p><small>- <a href="https://arxiv.org/pdf/1907.09615.pdf" target="_blank">arxiv.org/pdf/1907.09615.pdf</a></small></p>
					<aside class="notes">May be hard to observe if the change is possible. And even if it is, will it be within a short enough timeframe? Is there some promise from the system not to change the rules while you intervene by getting more money?</aside>
				</section>

				<!-- Ontology -->
				<section>
					<h1>Problems of ontology</h1>
				</section>
				<section>
					<h2>How do we define a measure for what's fair?</h2>
				</section>
				<section>
					<img class="plain" src="img/one-fairness-rule.jpg" height="550" />
				</section>
				<section>
					<img class="plain" src="img/sandwitch-chart.jpg" height="550" />
					<aside class="notes">This feels like the state of academia to me right now</aside>
				</section>
				<section data-background-image="img/blender.jpg">
				</section>
				<section>
					<blockquote>
						Recognition of the powerful pattern matching ability of humans is growing. As a result, humans are increasingly being deployed to make decisions that affect the well-being of other humans. 
					</blockquote>
					<p><small><a href="https://behavioralscientist.org/principles-for-the-application-of-human-intelligence/" target="_blank">behavioralscientist.org/principles-for-the-application-of-human-intelligence/</a></small></p>
					<aside class="notes">We already have problems with fairness and ethics from human decision makers. these problems only get amplified with machine decision systems.</aside>
				</section>
				<section>
					<p><em>Building ML systems in a corporate context...</em></p>
					<p class="fragment"><em>Also often true in 'pure research'...</em></p>
				</section>
				<section>
					<blockquote>Classification exerts a normative force (it's never purely descriptive)
					</blockquote>
					<aside class="notes">The boy and the bundle of sticks? Everything we tell people is a story they learn about themselves. Even if they reject the story, it becomes ingraned in their identity.</aside>
				</section>
				<section>
					<h3>Biometric Mirror</h3>
					<img src="img/biometric-mirror.jpg" height="450" />
					<p><small><a href="https://pursuit.unimelb.edu.au/articles/holding-a-black-mirror-up-to-artificial-intelligence" target="_blank">pursuit.unimelb.edu.au/articles/holding-a-black-mirror-up-to-artificial-intelligence</a></small></p>
				</section>
				<section>
					<blockquote>There is no prediction; <br />only intervention</blockquote>
					<aside class="notes">Can anyone give an example of building a prective model which didn't have any intended agency or intervention in the world? We predict things in order to do something about them.</aside>
				</section>
				<section>
					<h3>"Maths gloss"</h3>
					<aside class="notes">Anchoring phenomenon. Evidence showing that people accept a metric as more objective regardless of the </aside>
				</section>

				
				<section>
					<h1>In conclusion</h1>
				</section>
				<section>
					<img class="plain" src="img/chart.png" height="550" />
					<aside class="notes">Perhaps more than any tech before, we need interdisciplinary approaches. We can't think of our system as existing in a vacuum. We have to assess possible harms by thinking about how our system exists in a big web of other systems. </aside>
				</section>
				<section>
					<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In industry, it’s far too common there’s no discussion of societal harm or danger to freedom or rights. Just — can we make money on this? COOL. SHIP IT. I’ve worked alongside too many of these people, and I’m never going back. <a href="https://t.co/BEznnsobIC">https://t.co/BEznnsobIC</a></p>&mdash; Liz O’Sullivan (@lizjosullivan) <a href="https://twitter.com/lizjosullivan/status/1234544219026534401?ref_src=twsrc%5Etfw">March 2, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
				</section>
				<section>
					<h2>AI Ethics have to be defined</h2>
					<h3 style="text-align:left;" class="fragment">✅ in context</h3>
					<h3 style="text-align:left;" class="fragment">✅ grounded in intersectionality</h3>
					<h3 style="text-align:left;" class="fragment">✅ relative to other possible harms</h3>
				</section>
				<section>
					<section>
						<h1>Thank you!</h1>
						<h3>Questions?</h3>
						<p><small><a href="http://summerscope.github.io/slides/is-ai-ethics-just-ethics" target="_blank">summerscope.github.io/slides/is-ai-ethics-just-ethics</a></small></p>
					</section>
				</section>
				
			</div>
			<div class="twitter">
				<a href="https://twitter.com/summerscope" target="_blank">@summerscope</a>
			</div>
		</div>
		

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				backgroundTransition: 'zoom', // none/fade/slide/convex/concave/zoom
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true }
				]
			});
		</script>
	</body>
</html>
